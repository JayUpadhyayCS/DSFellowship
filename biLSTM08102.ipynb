{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow/Keras: 2.9.0\n",
      "pandas: 1.4.2\n",
      "numpy: 1.22.4\n",
      "sklearn: 1.1.1\n",
      "plotly: 5.9.0\n"
     ]
    }
   ],
   "source": [
    "# Tensorflow / Keras\n",
    "from tensorflow import keras # for building Neural Networks\n",
    "print('Tensorflow/Keras: %s' % keras.__version__) # print version\n",
    "from keras.models import Sequential # for creating a linear stack of layers for our Neural Network\n",
    "from keras import Input # for instantiating a keras tensor\n",
    "from keras.layers import Masking,Bidirectional, LSTM, RepeatVector, Dense, TimeDistributed,MaxPooling1D, Flatten, Conv1D,Conv2D,Dropout, MaxPooling2D, GRU # for creating layers inside the Neural Network\n",
    "from keras.optimizers import Adam , SGD\n",
    "from keras import metrics\n",
    "from keras.utils import plot_model\n",
    "# Data manipulation\n",
    "import pandas as pd # for data manipulation\n",
    "print('pandas: %s' % pd.__version__) # print version\n",
    "import numpy as np # for data manipulation\n",
    "print('numpy: %s' % np.__version__) # print version\n",
    "import numpy.ma as ma\n",
    "# Sklearn\n",
    "import sklearn\n",
    "print('sklearn: %s' % sklearn.__version__) # print version\n",
    "from sklearn.preprocessing import MinMaxScaler # for feature scaling\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Visualization\n",
    "import plotly \n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "print('plotly: %s' % plotly.__version__) # print version\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "#file accessing\n",
    "import os\n",
    "# time stuff\n",
    "from datetime import timedelta\n",
    "import calendar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defined the plot_the_model and plot_the_loss_curve functions.\n"
     ]
    }
   ],
   "source": [
    "#@title Define the plotting functions\n",
    "def plot_the_model(trained_weight, trained_bias, feature, label):\n",
    "  \"\"\"Plot the trained model against the training feature and label.\"\"\"\n",
    "\n",
    "  # Label the axes.\n",
    "  plt.xlabel(\"feature\")\n",
    "  plt.ylabel(\"label\")\n",
    "\n",
    "  # Plot the feature values vs. label values.\n",
    "  plt.scatter(feature, label)\n",
    "\n",
    "  # Create a red line representing the model. The red line starts\n",
    "  # at coordinates (x0, y0) and ends at coordinates (x1, y1).\n",
    "  x0 = 0\n",
    "  y0 = trained_bias\n",
    "  x1 = feature[-1]\n",
    "  y1 = trained_bias + (trained_weight * x1)\n",
    "  plt.plot([x0, x1], [y0, y1], c='r')\n",
    "\n",
    "  # Render the scatter plot and the red line.\n",
    "  plt.show()\n",
    "\n",
    "def plot_the_loss_curve(epochs, rmse):\n",
    "  \"\"\"Plot the loss curve, which shows loss vs. epoch.\"\"\"\n",
    "\n",
    "  plt.figure()\n",
    "  plt.xlabel(\"Epoch\")\n",
    "  plt.ylabel(\"Root Mean Squared Error\")\n",
    "  #epochs.remove(max(epochs))\n",
    "  \n",
    "  #rmse.drop(max(rmse))\n",
    "  plt.plot(epochs, rmse, label=\"Loss\")\n",
    "  plt.legend()\n",
    "  plt.ylim([min(rmse)*0.97, max(rmse)])\n",
    "  plt.show()\n",
    "\n",
    "print(\"Defined the plot_the_model and plot_the_loss_curve functions.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Read from file and establish dataframe and future functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('InSAR_data_south/displacement/export_dataframe1.csv')\n",
    "df=df.set_index([df.columns[0],df.columns[1]])\n",
    "df.columns=pd.to_datetime(df.columns, format='%Y%m%d')\n",
    "df=df.dropna(axis=0, how='all')#drop full nan rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Remove next line to include all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>2014-11-08</th>\n",
       "      <th>2014-12-02</th>\n",
       "      <th>2014-12-26</th>\n",
       "      <th>2015-02-12</th>\n",
       "      <th>2015-03-08</th>\n",
       "      <th>2015-04-01</th>\n",
       "      <th>2015-04-25</th>\n",
       "      <th>2015-05-19</th>\n",
       "      <th>2015-06-12</th>\n",
       "      <th>2015-07-06</th>\n",
       "      <th>...</th>\n",
       "      <th>2018-10-18</th>\n",
       "      <th>2018-10-30</th>\n",
       "      <th>2018-11-11</th>\n",
       "      <th>2018-11-17</th>\n",
       "      <th>2018-11-23</th>\n",
       "      <th>2018-12-05</th>\n",
       "      <th>2018-12-17</th>\n",
       "      <th>2018-12-29</th>\n",
       "      <th>2019-01-10</th>\n",
       "      <th>2019-01-22</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Longitude</th>\n",
       "      <th>Latitude</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"10\" valign=\"top\">-120.4087</th>\n",
       "      <th>36.9214</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-7.5666</td>\n",
       "      <td>-1.1857</td>\n",
       "      <td>1.2461</td>\n",
       "      <td>-2.0524</td>\n",
       "      <td>2.4467</td>\n",
       "      <td>-2.9248</td>\n",
       "      <td>-0.064559</td>\n",
       "      <td>0.90346</td>\n",
       "      <td>-2.5683</td>\n",
       "      <td>...</td>\n",
       "      <td>-27.043</td>\n",
       "      <td>-24.400</td>\n",
       "      <td>-25.155</td>\n",
       "      <td>-25.844</td>\n",
       "      <td>-29.027</td>\n",
       "      <td>-30.252</td>\n",
       "      <td>-27.912</td>\n",
       "      <td>-26.146</td>\n",
       "      <td>-26.877</td>\n",
       "      <td>-24.533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36.9234</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-7.5248</td>\n",
       "      <td>-1.1719</td>\n",
       "      <td>1.3339</td>\n",
       "      <td>-1.9606</td>\n",
       "      <td>2.5024</td>\n",
       "      <td>-2.8837</td>\n",
       "      <td>-0.134060</td>\n",
       "      <td>0.83566</td>\n",
       "      <td>-2.6301</td>\n",
       "      <td>...</td>\n",
       "      <td>-28.755</td>\n",
       "      <td>-26.130</td>\n",
       "      <td>-26.901</td>\n",
       "      <td>-27.640</td>\n",
       "      <td>-30.745</td>\n",
       "      <td>-31.948</td>\n",
       "      <td>-29.557</td>\n",
       "      <td>-27.866</td>\n",
       "      <td>-28.581</td>\n",
       "      <td>-26.237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36.9254</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-7.5343</td>\n",
       "      <td>-1.3051</td>\n",
       "      <td>1.1993</td>\n",
       "      <td>-2.0983</td>\n",
       "      <td>2.3195</td>\n",
       "      <td>-3.1164</td>\n",
       "      <td>-0.511280</td>\n",
       "      <td>0.44051</td>\n",
       "      <td>-3.0954</td>\n",
       "      <td>...</td>\n",
       "      <td>-31.703</td>\n",
       "      <td>-29.067</td>\n",
       "      <td>-29.805</td>\n",
       "      <td>-30.507</td>\n",
       "      <td>-33.684</td>\n",
       "      <td>-34.911</td>\n",
       "      <td>-32.592</td>\n",
       "      <td>-30.910</td>\n",
       "      <td>-31.643</td>\n",
       "      <td>-29.279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36.9274</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-7.4905</td>\n",
       "      <td>-1.2349</td>\n",
       "      <td>1.2690</td>\n",
       "      <td>-2.0480</td>\n",
       "      <td>2.3709</td>\n",
       "      <td>-3.0573</td>\n",
       "      <td>-0.513620</td>\n",
       "      <td>0.40750</td>\n",
       "      <td>-3.1829</td>\n",
       "      <td>...</td>\n",
       "      <td>-34.750</td>\n",
       "      <td>-32.103</td>\n",
       "      <td>-32.770</td>\n",
       "      <td>-33.508</td>\n",
       "      <td>-36.727</td>\n",
       "      <td>-37.891</td>\n",
       "      <td>-35.689</td>\n",
       "      <td>-34.034</td>\n",
       "      <td>-34.815</td>\n",
       "      <td>-32.406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36.9294</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-7.4614</td>\n",
       "      <td>-1.2258</td>\n",
       "      <td>1.2765</td>\n",
       "      <td>-2.0475</td>\n",
       "      <td>2.3984</td>\n",
       "      <td>-2.9988</td>\n",
       "      <td>-0.444090</td>\n",
       "      <td>0.47399</td>\n",
       "      <td>-3.0762</td>\n",
       "      <td>...</td>\n",
       "      <td>-35.470</td>\n",
       "      <td>-32.829</td>\n",
       "      <td>-33.479</td>\n",
       "      <td>-34.205</td>\n",
       "      <td>-37.386</td>\n",
       "      <td>-38.456</td>\n",
       "      <td>-36.252</td>\n",
       "      <td>-34.634</td>\n",
       "      <td>-35.449</td>\n",
       "      <td>-32.931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36.9314</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-7.4634</td>\n",
       "      <td>-1.1660</td>\n",
       "      <td>1.3789</td>\n",
       "      <td>-1.9943</td>\n",
       "      <td>2.4414</td>\n",
       "      <td>-2.8526</td>\n",
       "      <td>-0.350980</td>\n",
       "      <td>0.57990</td>\n",
       "      <td>-2.9256</td>\n",
       "      <td>...</td>\n",
       "      <td>-34.738</td>\n",
       "      <td>-32.112</td>\n",
       "      <td>-32.788</td>\n",
       "      <td>-33.488</td>\n",
       "      <td>-36.724</td>\n",
       "      <td>-37.750</td>\n",
       "      <td>-35.539</td>\n",
       "      <td>-33.964</td>\n",
       "      <td>-34.795</td>\n",
       "      <td>-32.236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36.9334</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-7.4255</td>\n",
       "      <td>-1.1440</td>\n",
       "      <td>1.4084</td>\n",
       "      <td>-1.9439</td>\n",
       "      <td>2.5082</td>\n",
       "      <td>-2.7706</td>\n",
       "      <td>-0.238070</td>\n",
       "      <td>0.67341</td>\n",
       "      <td>-2.7841</td>\n",
       "      <td>...</td>\n",
       "      <td>-33.479</td>\n",
       "      <td>-30.837</td>\n",
       "      <td>-31.484</td>\n",
       "      <td>-32.195</td>\n",
       "      <td>-35.461</td>\n",
       "      <td>-36.450</td>\n",
       "      <td>-34.290</td>\n",
       "      <td>-32.721</td>\n",
       "      <td>-33.556</td>\n",
       "      <td>-30.990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36.9354</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-7.4360</td>\n",
       "      <td>-1.1476</td>\n",
       "      <td>1.4514</td>\n",
       "      <td>-1.9075</td>\n",
       "      <td>2.5591</td>\n",
       "      <td>-2.7901</td>\n",
       "      <td>-0.236760</td>\n",
       "      <td>0.66471</td>\n",
       "      <td>-2.7504</td>\n",
       "      <td>...</td>\n",
       "      <td>-33.379</td>\n",
       "      <td>-30.735</td>\n",
       "      <td>-31.377</td>\n",
       "      <td>-32.099</td>\n",
       "      <td>-35.311</td>\n",
       "      <td>-36.275</td>\n",
       "      <td>-34.159</td>\n",
       "      <td>-32.558</td>\n",
       "      <td>-33.398</td>\n",
       "      <td>-30.780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36.9374</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-7.4381</td>\n",
       "      <td>-1.1158</td>\n",
       "      <td>1.5108</td>\n",
       "      <td>-1.8549</td>\n",
       "      <td>2.5910</td>\n",
       "      <td>-2.7065</td>\n",
       "      <td>-0.232510</td>\n",
       "      <td>0.65473</td>\n",
       "      <td>-2.7714</td>\n",
       "      <td>...</td>\n",
       "      <td>-33.089</td>\n",
       "      <td>-30.418</td>\n",
       "      <td>-31.089</td>\n",
       "      <td>-31.807</td>\n",
       "      <td>-34.990</td>\n",
       "      <td>-35.891</td>\n",
       "      <td>-33.817</td>\n",
       "      <td>-32.194</td>\n",
       "      <td>-33.007</td>\n",
       "      <td>-30.348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36.9394</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-7.4429</td>\n",
       "      <td>-1.0727</td>\n",
       "      <td>1.6068</td>\n",
       "      <td>-1.7088</td>\n",
       "      <td>2.6848</td>\n",
       "      <td>-2.6101</td>\n",
       "      <td>-0.073500</td>\n",
       "      <td>0.67181</td>\n",
       "      <td>-2.7176</td>\n",
       "      <td>...</td>\n",
       "      <td>-33.465</td>\n",
       "      <td>-30.738</td>\n",
       "      <td>-31.458</td>\n",
       "      <td>-32.147</td>\n",
       "      <td>-35.291</td>\n",
       "      <td>-36.132</td>\n",
       "      <td>-34.065</td>\n",
       "      <td>-32.425</td>\n",
       "      <td>-33.193</td>\n",
       "      <td>-30.520</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 110 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    2014-11-08  2014-12-02  2014-12-26  2015-02-12  \\\n",
       "Longitude Latitude                                                   \n",
       "-120.4087 36.9214          0.0     -7.5666     -1.1857      1.2461   \n",
       "          36.9234          0.0     -7.5248     -1.1719      1.3339   \n",
       "          36.9254          0.0     -7.5343     -1.3051      1.1993   \n",
       "          36.9274          0.0     -7.4905     -1.2349      1.2690   \n",
       "          36.9294          0.0     -7.4614     -1.2258      1.2765   \n",
       "          36.9314          0.0     -7.4634     -1.1660      1.3789   \n",
       "          36.9334          0.0     -7.4255     -1.1440      1.4084   \n",
       "          36.9354          0.0     -7.4360     -1.1476      1.4514   \n",
       "          36.9374          0.0     -7.4381     -1.1158      1.5108   \n",
       "          36.9394          0.0     -7.4429     -1.0727      1.6068   \n",
       "\n",
       "                    2015-03-08  2015-04-01  2015-04-25  2015-05-19  \\\n",
       "Longitude Latitude                                                   \n",
       "-120.4087 36.9214      -2.0524      2.4467     -2.9248   -0.064559   \n",
       "          36.9234      -1.9606      2.5024     -2.8837   -0.134060   \n",
       "          36.9254      -2.0983      2.3195     -3.1164   -0.511280   \n",
       "          36.9274      -2.0480      2.3709     -3.0573   -0.513620   \n",
       "          36.9294      -2.0475      2.3984     -2.9988   -0.444090   \n",
       "          36.9314      -1.9943      2.4414     -2.8526   -0.350980   \n",
       "          36.9334      -1.9439      2.5082     -2.7706   -0.238070   \n",
       "          36.9354      -1.9075      2.5591     -2.7901   -0.236760   \n",
       "          36.9374      -1.8549      2.5910     -2.7065   -0.232510   \n",
       "          36.9394      -1.7088      2.6848     -2.6101   -0.073500   \n",
       "\n",
       "                    2015-06-12  2015-07-06  ...  2018-10-18  2018-10-30  \\\n",
       "Longitude Latitude                          ...                           \n",
       "-120.4087 36.9214      0.90346     -2.5683  ...     -27.043     -24.400   \n",
       "          36.9234      0.83566     -2.6301  ...     -28.755     -26.130   \n",
       "          36.9254      0.44051     -3.0954  ...     -31.703     -29.067   \n",
       "          36.9274      0.40750     -3.1829  ...     -34.750     -32.103   \n",
       "          36.9294      0.47399     -3.0762  ...     -35.470     -32.829   \n",
       "          36.9314      0.57990     -2.9256  ...     -34.738     -32.112   \n",
       "          36.9334      0.67341     -2.7841  ...     -33.479     -30.837   \n",
       "          36.9354      0.66471     -2.7504  ...     -33.379     -30.735   \n",
       "          36.9374      0.65473     -2.7714  ...     -33.089     -30.418   \n",
       "          36.9394      0.67181     -2.7176  ...     -33.465     -30.738   \n",
       "\n",
       "                    2018-11-11  2018-11-17  2018-11-23  2018-12-05  \\\n",
       "Longitude Latitude                                                   \n",
       "-120.4087 36.9214      -25.155     -25.844     -29.027     -30.252   \n",
       "          36.9234      -26.901     -27.640     -30.745     -31.948   \n",
       "          36.9254      -29.805     -30.507     -33.684     -34.911   \n",
       "          36.9274      -32.770     -33.508     -36.727     -37.891   \n",
       "          36.9294      -33.479     -34.205     -37.386     -38.456   \n",
       "          36.9314      -32.788     -33.488     -36.724     -37.750   \n",
       "          36.9334      -31.484     -32.195     -35.461     -36.450   \n",
       "          36.9354      -31.377     -32.099     -35.311     -36.275   \n",
       "          36.9374      -31.089     -31.807     -34.990     -35.891   \n",
       "          36.9394      -31.458     -32.147     -35.291     -36.132   \n",
       "\n",
       "                    2018-12-17  2018-12-29  2019-01-10  2019-01-22  \n",
       "Longitude Latitude                                                  \n",
       "-120.4087 36.9214      -27.912     -26.146     -26.877     -24.533  \n",
       "          36.9234      -29.557     -27.866     -28.581     -26.237  \n",
       "          36.9254      -32.592     -30.910     -31.643     -29.279  \n",
       "          36.9274      -35.689     -34.034     -34.815     -32.406  \n",
       "          36.9294      -36.252     -34.634     -35.449     -32.931  \n",
       "          36.9314      -35.539     -33.964     -34.795     -32.236  \n",
       "          36.9334      -34.290     -32.721     -33.556     -30.990  \n",
       "          36.9354      -34.159     -32.558     -33.398     -30.780  \n",
       "          36.9374      -33.817     -32.194     -33.007     -30.348  \n",
       "          36.9394      -34.065     -32.425     -33.193     -30.520  \n",
       "\n",
       "[10 rows x 110 columns]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#df=df.iloc[100000:500000] #cuts data to long 120-119 approx\n",
    "#df=df.iloc[200000:200050]\n",
    "df=df.iloc[200880:200890]\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tuple"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2=pd.read_csv('InSAR_data_south/displacement/groundwater.csv')\n",
    "df2=df2.set_index([df2.columns[0],df2.columns[1]])\n",
    "df2.columns=pd.to_datetime(df2.columns, format='%Y-%m-%d')\n",
    "df2=df2.dropna(axis=0, how='all')#drop full nan rows\n",
    "for y in range(df.columns.size):# includes all dates and adds nans\n",
    "    if df.columns[y] != df2.columns[y]: #if column is misssing insert it\n",
    "        df2.insert(loc=y,column=df.columns[y],value=np.nan*11286,allow_duplicates=False)\n",
    "df2.index[0]\n",
    "type(df2.index[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11286"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.index.size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>2014-11-08</th>\n",
       "      <th>2014-12-02</th>\n",
       "      <th>2014-12-26</th>\n",
       "      <th>2015-02-12</th>\n",
       "      <th>2015-03-08</th>\n",
       "      <th>2015-04-01</th>\n",
       "      <th>2015-04-25</th>\n",
       "      <th>2015-05-19</th>\n",
       "      <th>2015-06-12</th>\n",
       "      <th>2015-07-06</th>\n",
       "      <th>...</th>\n",
       "      <th>2018-10-18</th>\n",
       "      <th>2018-10-30</th>\n",
       "      <th>2018-11-11</th>\n",
       "      <th>2018-11-17</th>\n",
       "      <th>2018-11-23</th>\n",
       "      <th>2018-12-05</th>\n",
       "      <th>2018-12-17</th>\n",
       "      <th>2018-12-29</th>\n",
       "      <th>2019-01-10</th>\n",
       "      <th>2019-01-22</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Longitude</th>\n",
       "      <th>Latitude</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>-120.9486</th>\n",
       "      <th>37.6316</th>\n",
       "      <td>81.000000</td>\n",
       "      <td>81.000000</td>\n",
       "      <td>81.000000</td>\n",
       "      <td>81.000000</td>\n",
       "      <td>81.000000</td>\n",
       "      <td>81.000000</td>\n",
       "      <td>81.000000</td>\n",
       "      <td>81.000000</td>\n",
       "      <td>81.000000</td>\n",
       "      <td>81.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>75.653992</td>\n",
       "      <td>75.813688</td>\n",
       "      <td>75.973384</td>\n",
       "      <td>76.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>76.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-120.8926</th>\n",
       "      <th>36.7114</th>\n",
       "      <td>72.248964</td>\n",
       "      <td>72.248964</td>\n",
       "      <td>72.248964</td>\n",
       "      <td>72.248964</td>\n",
       "      <td>72.248964</td>\n",
       "      <td>72.248964</td>\n",
       "      <td>72.248964</td>\n",
       "      <td>72.248964</td>\n",
       "      <td>72.248964</td>\n",
       "      <td>72.248964</td>\n",
       "      <td>...</td>\n",
       "      <td>75.200000</td>\n",
       "      <td>75.200000</td>\n",
       "      <td>75.200000</td>\n",
       "      <td>75.2</td>\n",
       "      <td>75.2</td>\n",
       "      <td>75.2</td>\n",
       "      <td>75.2</td>\n",
       "      <td>75.2</td>\n",
       "      <td>75.2</td>\n",
       "      <td>75.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-120.8846</th>\n",
       "      <th>36.6994</th>\n",
       "      <td>72.248964</td>\n",
       "      <td>72.248964</td>\n",
       "      <td>72.248964</td>\n",
       "      <td>72.248964</td>\n",
       "      <td>72.248964</td>\n",
       "      <td>72.248964</td>\n",
       "      <td>72.248964</td>\n",
       "      <td>72.248964</td>\n",
       "      <td>72.248964</td>\n",
       "      <td>72.248964</td>\n",
       "      <td>...</td>\n",
       "      <td>75.200000</td>\n",
       "      <td>75.200000</td>\n",
       "      <td>75.200000</td>\n",
       "      <td>75.2</td>\n",
       "      <td>75.2</td>\n",
       "      <td>75.2</td>\n",
       "      <td>75.2</td>\n",
       "      <td>75.2</td>\n",
       "      <td>75.2</td>\n",
       "      <td>75.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-120.8746</th>\n",
       "      <th>36.6874</th>\n",
       "      <td>72.248964</td>\n",
       "      <td>72.248964</td>\n",
       "      <td>72.248964</td>\n",
       "      <td>72.248964</td>\n",
       "      <td>72.248964</td>\n",
       "      <td>72.248964</td>\n",
       "      <td>72.248964</td>\n",
       "      <td>72.248964</td>\n",
       "      <td>72.248964</td>\n",
       "      <td>72.248964</td>\n",
       "      <td>...</td>\n",
       "      <td>75.200000</td>\n",
       "      <td>75.200000</td>\n",
       "      <td>75.200000</td>\n",
       "      <td>75.2</td>\n",
       "      <td>75.2</td>\n",
       "      <td>75.2</td>\n",
       "      <td>75.2</td>\n",
       "      <td>75.2</td>\n",
       "      <td>75.2</td>\n",
       "      <td>75.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-120.1368</th>\n",
       "      <th>35.6991</th>\n",
       "      <td>141.123387</td>\n",
       "      <td>141.123387</td>\n",
       "      <td>141.123387</td>\n",
       "      <td>141.123387</td>\n",
       "      <td>141.123387</td>\n",
       "      <td>141.123387</td>\n",
       "      <td>141.123387</td>\n",
       "      <td>141.123387</td>\n",
       "      <td>141.123387</td>\n",
       "      <td>141.123387</td>\n",
       "      <td>...</td>\n",
       "      <td>150.898190</td>\n",
       "      <td>151.000000</td>\n",
       "      <td>151.000000</td>\n",
       "      <td>151.0</td>\n",
       "      <td>151.0</td>\n",
       "      <td>151.0</td>\n",
       "      <td>151.0</td>\n",
       "      <td>151.0</td>\n",
       "      <td>151.0</td>\n",
       "      <td>151.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-119.0351</th>\n",
       "      <th>36.3933</th>\n",
       "      <td>17.409091</td>\n",
       "      <td>17.409091</td>\n",
       "      <td>17.409091</td>\n",
       "      <td>17.409091</td>\n",
       "      <td>17.409091</td>\n",
       "      <td>17.409091</td>\n",
       "      <td>17.409091</td>\n",
       "      <td>17.409091</td>\n",
       "      <td>17.409091</td>\n",
       "      <td>17.409091</td>\n",
       "      <td>...</td>\n",
       "      <td>16.978182</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>17.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>17.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-119.0271</th>\n",
       "      <th>36.3793</th>\n",
       "      <td>17.409091</td>\n",
       "      <td>17.409091</td>\n",
       "      <td>17.409091</td>\n",
       "      <td>17.409091</td>\n",
       "      <td>17.409091</td>\n",
       "      <td>17.409091</td>\n",
       "      <td>17.409091</td>\n",
       "      <td>17.409091</td>\n",
       "      <td>17.409091</td>\n",
       "      <td>17.409091</td>\n",
       "      <td>...</td>\n",
       "      <td>16.978182</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>17.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>17.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-119.0571</th>\n",
       "      <th>36.4493</th>\n",
       "      <td>17.409091</td>\n",
       "      <td>17.409091</td>\n",
       "      <td>17.409091</td>\n",
       "      <td>17.409091</td>\n",
       "      <td>17.409091</td>\n",
       "      <td>17.409091</td>\n",
       "      <td>17.409091</td>\n",
       "      <td>17.409091</td>\n",
       "      <td>17.409091</td>\n",
       "      <td>17.409091</td>\n",
       "      <td>...</td>\n",
       "      <td>16.978182</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>17.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>17.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-119.0211</th>\n",
       "      <th>36.3993</th>\n",
       "      <td>17.409091</td>\n",
       "      <td>17.409091</td>\n",
       "      <td>17.409091</td>\n",
       "      <td>17.409091</td>\n",
       "      <td>17.409091</td>\n",
       "      <td>17.409091</td>\n",
       "      <td>17.409091</td>\n",
       "      <td>17.409091</td>\n",
       "      <td>17.409091</td>\n",
       "      <td>17.409091</td>\n",
       "      <td>...</td>\n",
       "      <td>16.978182</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>17.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>17.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-119.0111</th>\n",
       "      <th>36.3873</th>\n",
       "      <td>17.409091</td>\n",
       "      <td>17.409091</td>\n",
       "      <td>17.409091</td>\n",
       "      <td>17.409091</td>\n",
       "      <td>17.409091</td>\n",
       "      <td>17.409091</td>\n",
       "      <td>17.409091</td>\n",
       "      <td>17.409091</td>\n",
       "      <td>17.409091</td>\n",
       "      <td>17.409091</td>\n",
       "      <td>...</td>\n",
       "      <td>16.978182</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>17.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>17.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11286 rows × 110 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    2014-11-08  2014-12-02  2014-12-26  2015-02-12  \\\n",
       "Longitude Latitude                                                   \n",
       "-120.9486 37.6316    81.000000   81.000000   81.000000   81.000000   \n",
       "-120.8926 36.7114    72.248964   72.248964   72.248964   72.248964   \n",
       "-120.8846 36.6994    72.248964   72.248964   72.248964   72.248964   \n",
       "-120.8746 36.6874    72.248964   72.248964   72.248964   72.248964   \n",
       "-120.1368 35.6991   141.123387  141.123387  141.123387  141.123387   \n",
       "...                        ...         ...         ...         ...   \n",
       "-119.0351 36.3933    17.409091   17.409091   17.409091   17.409091   \n",
       "-119.0271 36.3793    17.409091   17.409091   17.409091   17.409091   \n",
       "-119.0571 36.4493    17.409091   17.409091   17.409091   17.409091   \n",
       "-119.0211 36.3993    17.409091   17.409091   17.409091   17.409091   \n",
       "-119.0111 36.3873    17.409091   17.409091   17.409091   17.409091   \n",
       "\n",
       "                    2015-03-08  2015-04-01  2015-04-25  2015-05-19  \\\n",
       "Longitude Latitude                                                   \n",
       "-120.9486 37.6316    81.000000   81.000000   81.000000   81.000000   \n",
       "-120.8926 36.7114    72.248964   72.248964   72.248964   72.248964   \n",
       "-120.8846 36.6994    72.248964   72.248964   72.248964   72.248964   \n",
       "-120.8746 36.6874    72.248964   72.248964   72.248964   72.248964   \n",
       "-120.1368 35.6991   141.123387  141.123387  141.123387  141.123387   \n",
       "...                        ...         ...         ...         ...   \n",
       "-119.0351 36.3933    17.409091   17.409091   17.409091   17.409091   \n",
       "-119.0271 36.3793    17.409091   17.409091   17.409091   17.409091   \n",
       "-119.0571 36.4493    17.409091   17.409091   17.409091   17.409091   \n",
       "-119.0211 36.3993    17.409091   17.409091   17.409091   17.409091   \n",
       "-119.0111 36.3873    17.409091   17.409091   17.409091   17.409091   \n",
       "\n",
       "                    2015-06-12  2015-07-06  ...  2018-10-18  2018-10-30  \\\n",
       "Longitude Latitude                          ...                           \n",
       "-120.9486 37.6316    81.000000   81.000000  ...   75.653992   75.813688   \n",
       "-120.8926 36.7114    72.248964   72.248964  ...   75.200000   75.200000   \n",
       "-120.8846 36.6994    72.248964   72.248964  ...   75.200000   75.200000   \n",
       "-120.8746 36.6874    72.248964   72.248964  ...   75.200000   75.200000   \n",
       "-120.1368 35.6991   141.123387  141.123387  ...  150.898190  151.000000   \n",
       "...                        ...         ...  ...         ...         ...   \n",
       "-119.0351 36.3933    17.409091   17.409091  ...   16.978182   17.000000   \n",
       "-119.0271 36.3793    17.409091   17.409091  ...   16.978182   17.000000   \n",
       "-119.0571 36.4493    17.409091   17.409091  ...   16.978182   17.000000   \n",
       "-119.0211 36.3993    17.409091   17.409091  ...   16.978182   17.000000   \n",
       "-119.0111 36.3873    17.409091   17.409091  ...   16.978182   17.000000   \n",
       "\n",
       "                    2018-11-11  2018-11-17  2018-11-23  2018-12-05  \\\n",
       "Longitude Latitude                                                   \n",
       "-120.9486 37.6316    75.973384        76.0        76.0        76.0   \n",
       "-120.8926 36.7114    75.200000        75.2        75.2        75.2   \n",
       "-120.8846 36.6994    75.200000        75.2        75.2        75.2   \n",
       "-120.8746 36.6874    75.200000        75.2        75.2        75.2   \n",
       "-120.1368 35.6991   151.000000       151.0       151.0       151.0   \n",
       "...                        ...         ...         ...         ...   \n",
       "-119.0351 36.3933    17.000000        17.0        17.0        17.0   \n",
       "-119.0271 36.3793    17.000000        17.0        17.0        17.0   \n",
       "-119.0571 36.4493    17.000000        17.0        17.0        17.0   \n",
       "-119.0211 36.3993    17.000000        17.0        17.0        17.0   \n",
       "-119.0111 36.3873    17.000000        17.0        17.0        17.0   \n",
       "\n",
       "                    2018-12-17  2018-12-29  2019-01-10  2019-01-22  \n",
       "Longitude Latitude                                                  \n",
       "-120.9486 37.6316         76.0        76.0        76.0        76.0  \n",
       "-120.8926 36.7114         75.2        75.2        75.2        75.2  \n",
       "-120.8846 36.6994         75.2        75.2        75.2        75.2  \n",
       "-120.8746 36.6874         75.2        75.2        75.2        75.2  \n",
       "-120.1368 35.6991        151.0       151.0       151.0       151.0  \n",
       "...                        ...         ...         ...         ...  \n",
       "-119.0351 36.3933         17.0        17.0        17.0        17.0  \n",
       "-119.0271 36.3793         17.0        17.0        17.0        17.0  \n",
       "-119.0571 36.4493         17.0        17.0        17.0        17.0  \n",
       "-119.0211 36.3993         17.0        17.0        17.0        17.0  \n",
       "-119.0111 36.3873         17.0        17.0        17.0        17.0  \n",
       "\n",
       "[11286 rows x 110 columns]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for x in range(df2.index.size): #for each row\n",
    "    curr=np.nan\n",
    "    for y in range(df2.columns.size): #for each value in the row\n",
    "        n=0\n",
    "        while np.isnan(curr): # if saved value is empty\n",
    "            curr=df2.iloc[x].values[n] #look for next actual value\n",
    "            n+=1\n",
    "        if np.isnan(df2.iloc[x].values[y]):#if data is nan then replace with previous curr value\n",
    "            df2.iat[x,y]=curr\n",
    "            #df2.iloc[x].values[y]=curr\n",
    "        else:\n",
    "            curr= df2.iloc[x].values[y]\n",
    "                \n",
    "\n",
    "df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Timestep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestep=10\n",
    "# 1 2 3 4 5 6 7 ...\n",
    "# w/ timestep of 2\n",
    "# [[1,2][2,3][3,4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shaping function (remove start and step for first for loop to include all data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shaping(datain, timestep):\n",
    "    #print(arr)\n",
    "    cnt=0\n",
    "    for row in range(len(datain.index)): #picks a row at every iteration, allows to reduction of input and inclusion of multiple time series, remove start and step to run on full dataset \n",
    "    # Convert input dataframe to array and flatten\n",
    "        #datain.iloc[row].fillna(datain.iloc[row].mean).to_numpy().flatten()\n",
    "        \n",
    "        arr=datain.iloc[row].to_numpy().flatten() # flatten row\n",
    "        #arr=arr.fillna(arr.mean)\n",
    "        arr=np.where(np.isnan(arr), ma.array(arr, mask=np.isnan(arr)).mean(), arr) \n",
    "        for mth in range(0, len(datain.columns)-(2*timestep)+1): # Define range lenght of the dates - 2* amount of timesep?? +1\n",
    "            cnt=cnt+1 # Gives us the number of samples. Later used to reshape the data\n",
    "            X_start=mth # Start month for inputs of each sample\n",
    "            X_end=mth+timestep # End month for inputs of each sample\n",
    "            Y_start=mth+timestep # Start month for targets of each sample. Note, start is inclusive and end is exclusive, that's why X_end and Y_start is the same number\n",
    "            Y_end=mth+2*timestep # End month for targets of each sample.  \n",
    "            \n",
    "            # Assemble input and target arrays containing all samples\n",
    "            if cnt==1:\n",
    "                X_comb=arr[X_start:X_end]\n",
    "                Y_comb=arr[Y_start:Y_end]\n",
    "            else: \n",
    "                X_comb=np.append(X_comb, arr[X_start:X_end])\n",
    "                Y_comb=np.append(Y_comb, arr[Y_start:Y_end])\n",
    "    print(X_comb.shape)\n",
    "    # Reshape input and target arrays\n",
    "    X_out=np.reshape(X_comb, (cnt, timestep, 1))\n",
    "    Y_out=np.reshape(Y_comb, (cnt, timestep, 1))\n",
    "    return X_out, Y_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shaping2(datain,datain2, timestep):\n",
    "    #print(arr)\n",
    "    cnt=0\n",
    "    for row in datain2.index: #picks a row at every iteration, allows to reduction of input and inclusion of multiple time series, remove start and step to run on full dataset \n",
    "    # Convert input dataframe to array and flatten\n",
    "        #datain.iloc[row].fillna(datain.iloc[row].mean).to_numpy().flatten()\n",
    "        arr=datain.loc[row].to_numpy().flatten() # flatten row\n",
    "        arr2=datain2.loc[row].to_numpy().flatten()\n",
    "        arr3=np.concatenate((arr,arr2)).reshape(2,110)\n",
    "        #arr=arr.fillna(arr.mean)\n",
    "        arr=np.where(np.isnan(arr), ma.array(arr, mask=np.isnan(arr)).mean(), arr) \n",
    "        for mth in range(0, len(datain.columns)-(2*timestep)+1): # Define range lenght of the dates - 2* amount of timesep?? +1\n",
    "            cnt=cnt+1 # Gives us the number of samples. Later used to reshape the data\n",
    "            X_start=mth # Start month for inputs of each sample\n",
    "            X_end=mth+timestep # End month for inputs of each sample\n",
    "            Y_start=mth+timestep # Start month for targets of each sample. Note, start is inclusive and end is exclusive, that's why X_end and Y_start is the same number\n",
    "            Y_end=mth+2*timestep # End month for targets of each sample.  \n",
    "            \n",
    "            # Assemble input and target arrays containing all samples\n",
    "            if cnt==1:\n",
    "                X_comb=arr[X_start:X_end]\n",
    "                Y_comb=arr[0][Y_start:Y_end]\n",
    "            else: \n",
    "                X_comb=np.append(X_comb, arr[X_start:X_end])\n",
    "                Y_comb=np.append(Y_comb, arr[0][Y_start:Y_end])\n",
    "    \n",
    "    # Reshape input and target arrays\n",
    "    X_out=np.reshape(X_comb, (cnt, timestep, 2))\n",
    "    Y_out=np.reshape(Y_comb, (cnt, timestep, 2))\n",
    "    return X_out, Y_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 2 - Split and Shape data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7280,)\n",
      "(1820,)\n"
     ]
    }
   ],
   "source": [
    "#Divides data into 80% training 20% testing\n",
    "#train, test = train_test_split(df, test_size=0.2)\n",
    "# creates sequences to train\n",
    "# X , Y = shaping(datain=df, timestep= timestep )\n",
    "# X_train, X_test, Y_train, Y_test = train_test_split(X,Y , test_size=0.2, random_state=28)\n",
    "# nsamples, nx, ny = X_train.shape\n",
    "# X_train = X_train.reshape((nsamples,nx*ny))\n",
    "# nsamples, nx, ny = Y_train.shape\n",
    "# Y_train = Y_train.reshape((nsamples,nx*ny))\n",
    "# nsamples, nx, ny = X_test.shape\n",
    "# X_test = X_test.reshape((nsamples,nx*ny))\n",
    "# nsamples, nx, ny = Y_test.shape\n",
    "# Y_test = Y_test.reshape((nsamples,nx*ny))\n",
    "#Divides data into 80% training 20% testing\n",
    "\n",
    "train, test = train_test_split(df, test_size=0.2, random_state=64, shuffle=False)\n",
    "# creates sequences to train\n",
    "X_train, Y_train = shaping(datain=train, timestep= timestep )\n",
    "X_test, Y_test = shaping(datain=test, timestep=timestep)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found array with dim 3. MinMaxScaler expected <= 2.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32me:\\DSFellowship\\biLSTM0810.ipynb Cell 17\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/DSFellowship/biLSTM0810.ipynb#X21sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m scaler \u001b[39m=\u001b[39m MinMaxScaler()\n\u001b[1;32m----> <a href='vscode-notebook-cell:/e%3A/DSFellowship/biLSTM0810.ipynb#X21sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m X_train \u001b[39m=\u001b[39m scaler\u001b[39m.\u001b[39;49mfit_transform(X_train)\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/DSFellowship/biLSTM0810.ipynb#X21sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m X_test \u001b[39m=\u001b[39m scaler\u001b[39m.\u001b[39mtransform(X_test)\n",
      "File \u001b[1;32mc:\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:867\u001b[0m, in \u001b[0;36mTransformerMixin.fit_transform\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    863\u001b[0m \u001b[39m# non-optimized default implementation; override when a better\u001b[39;00m\n\u001b[0;32m    864\u001b[0m \u001b[39m# method is possible for a given clustering algorithm\u001b[39;00m\n\u001b[0;32m    865\u001b[0m \u001b[39mif\u001b[39;00m y \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    866\u001b[0m     \u001b[39m# fit method of arity 1 (unsupervised transformation)\u001b[39;00m\n\u001b[1;32m--> 867\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfit(X, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\u001b[39m.\u001b[39mtransform(X)\n\u001b[0;32m    868\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    869\u001b[0m     \u001b[39m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfit(X, y, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\u001b[39m.\u001b[39mtransform(X)\n",
      "File \u001b[1;32mc:\\Python\\Python310\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:420\u001b[0m, in \u001b[0;36mMinMaxScaler.fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    418\u001b[0m \u001b[39m# Reset internal state before fitting\u001b[39;00m\n\u001b[0;32m    419\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()\n\u001b[1;32m--> 420\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpartial_fit(X, y)\n",
      "File \u001b[1;32mc:\\Python\\Python310\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:457\u001b[0m, in \u001b[0;36mMinMaxScaler.partial_fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    451\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[0;32m    452\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mMinMaxScaler does not support sparse input. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    453\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mConsider using MaxAbsScaler instead.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    454\u001b[0m     )\n\u001b[0;32m    456\u001b[0m first_pass \u001b[39m=\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mn_samples_seen_\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 457\u001b[0m X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_data(\n\u001b[0;32m    458\u001b[0m     X,\n\u001b[0;32m    459\u001b[0m     reset\u001b[39m=\u001b[39;49mfirst_pass,\n\u001b[0;32m    460\u001b[0m     dtype\u001b[39m=\u001b[39;49mFLOAT_DTYPES,\n\u001b[0;32m    461\u001b[0m     force_all_finite\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mallow-nan\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    462\u001b[0m )\n\u001b[0;32m    464\u001b[0m data_min \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mnanmin(X, axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[0;32m    465\u001b[0m data_max \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mnanmax(X, axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n",
      "File \u001b[1;32mc:\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:577\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    575\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mValidation should be done on X, y or both.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    576\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m no_val_y:\n\u001b[1;32m--> 577\u001b[0m     X \u001b[39m=\u001b[39m check_array(X, input_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mX\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcheck_params)\n\u001b[0;32m    578\u001b[0m     out \u001b[39m=\u001b[39m X\n\u001b[0;32m    579\u001b[0m \u001b[39melif\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_y:\n",
      "File \u001b[1;32mc:\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:893\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    887\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    888\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mdtype=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mnumeric\u001b[39m\u001b[39m'\u001b[39m\u001b[39m is not compatible with arrays of bytes/strings.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    889\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mConvert your data to numeric values explicitly instead.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    890\u001b[0m     )\n\u001b[0;32m    892\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m allow_nd \u001b[39mand\u001b[39;00m array\u001b[39m.\u001b[39mndim \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m3\u001b[39m:\n\u001b[1;32m--> 893\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    894\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFound array with dim \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m expected <= 2.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    895\u001b[0m         \u001b[39m%\u001b[39m (array\u001b[39m.\u001b[39mndim, estimator_name)\n\u001b[0;32m    896\u001b[0m     )\n\u001b[0;32m    898\u001b[0m \u001b[39mif\u001b[39;00m force_all_finite:\n\u001b[0;32m    899\u001b[0m     _assert_all_finite(\n\u001b[0;32m    900\u001b[0m         array,\n\u001b[0;32m    901\u001b[0m         input_name\u001b[39m=\u001b[39minput_name,\n\u001b[0;32m    902\u001b[0m         estimator_name\u001b[39m=\u001b[39mestimator_name,\n\u001b[0;32m    903\u001b[0m         allow_nan\u001b[39m=\u001b[39mforce_all_finite \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mallow-nan\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    904\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Found array with dim 3. MinMaxScaler expected <= 2."
     ]
    }
   ],
   "source": [
    "scaler = MinMaxScaler()\n",
    "\n",
    "X_train = scaler.fit_transform(arr2.reshape(-1, arr2.shape[-1])).reshape(arr2.shape)\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "\n",
    "X_test = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[  0.      ],\n",
       "        [ -7.5666  ],\n",
       "        [ -1.1857  ],\n",
       "        ...,\n",
       "        [ -0.064559],\n",
       "        [  0.90346 ],\n",
       "        [ -2.5683  ]],\n",
       "\n",
       "       [[ -7.5666  ],\n",
       "        [ -1.1857  ],\n",
       "        [  1.2461  ],\n",
       "        ...,\n",
       "        [  0.90346 ],\n",
       "        [ -2.5683  ],\n",
       "        [ -1.6073  ]],\n",
       "\n",
       "       [[ -1.1857  ],\n",
       "        [  1.2461  ],\n",
       "        [ -2.0524  ],\n",
       "        ...,\n",
       "        [ -2.5683  ],\n",
       "        [ -1.6073  ],\n",
       "        [ -7.5252  ]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[-27.634   ],\n",
       "        [-24.497   ],\n",
       "        [-27.151   ],\n",
       "        ...,\n",
       "        [-29.259   ],\n",
       "        [-29.738   ],\n",
       "        [-30.216   ]],\n",
       "\n",
       "       [[-24.497   ],\n",
       "        [-27.151   ],\n",
       "        [-26.004   ],\n",
       "        ...,\n",
       "        [-29.738   ],\n",
       "        [-30.216   ],\n",
       "        [-28.796   ]],\n",
       "\n",
       "       [[-27.151   ],\n",
       "        [-26.004   ],\n",
       "        [-28.013   ],\n",
       "        ...,\n",
       "        [-30.216   ],\n",
       "        [-28.796   ],\n",
       "        [-31.16    ]]])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.      ],\n",
       "       [-7.5666  ],\n",
       "       [-1.1857  ],\n",
       "       [ 1.2461  ],\n",
       "       [-2.0524  ],\n",
       "       [ 2.4467  ],\n",
       "       [-2.9248  ],\n",
       "       [-0.064559],\n",
       "       [ 0.90346 ],\n",
       "       [-2.5683  ]])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-7.5666  ],\n",
       "       [-1.1857  ],\n",
       "       [ 1.2461  ],\n",
       "       [-2.0524  ],\n",
       "       [ 2.4467  ],\n",
       "       [-2.9248  ],\n",
       "       [-0.064559],\n",
       "       [ 0.90346 ],\n",
       "       [-2.5683  ],\n",
       "       [-1.6073  ]])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 3 - Specify the structure of a Neural Network, first model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = Sequential(name=\"biLSTM-Model\") # Model\n",
    "#model2.add(Masking(mask_value=-1000, input_shape=(timestep, 1)))\n",
    "model2.add(Input(shape=(X_train.shape[1],X_train.shape[2]), name='Input-Layer')) # Input Layer - need to speicfy the shape of inputs\n",
    "model2.add(Bidirectional(LSTM(units=64, activation='relu', recurrent_activation='sigmoid', stateful=False), name='Hidden-LSTM-Encoder-Layer')) # Encoder Layer\n",
    "model2.add(RepeatVector(Y_train.shape[1], name='Repeat-Vector-Layer')) # Repeat Vector\n",
    "model2.add(Bidirectional(LSTM(units=64, activation='relu', recurrent_activation='sigmoid', stateful=False, return_sequences=True), name='Hidden-LSTM-Decoder-Layer')) # Decoder Layer\n",
    "model2.add(TimeDistributed(Dense(units=1, activation='linear'), name='Output-Layer')) # Output Layer, Linear(x) = x\n",
    "#optimizer=Adam(.005)\n",
    "model2.compile(optimizer='adam', # default='rmsprop', an algorithm to be used in backpropagation\n",
    "              loss='mean_squared_error', # Loss function to be optimized. A string (name of loss function), or a tf.keras.losses.Loss instance.\n",
    "              metrics=['MeanSquaredError', 'accuracy'], # List of metrics to be evaluated by the model during training and testing. Each of this can be a string (name of a built-in function), function or a tf.keras.metrics.Metric instance. \n",
    "              loss_weights=None, # default=None, Optional list or dictionary specifying scalar coefficients (Python floats) to weight the loss contributions of different model outputs.\n",
    "              weighted_metrics=None, # default=None, List of metrics to be evaluated and weighted by sample_weight or class_weight during training and testing.\n",
    "              run_eagerly=None, # Defaults to False. If True, this Model's logic will not be wrapped in a tf.function. Recommended to leave this as None unless your Model cannot be run inside a tf.function.\n",
    "              steps_per_execution=None # Defaults to 1. The number of batches to run during each tf.function call. Running multiple batches inside a single tf.function call can greatly improve performance on TPUs or small models with a large Python overhead.\n",
    "        \n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/250\n",
      "291/291 - 6s - loss: 28.4334 - mean_squared_error: 28.4334 - accuracy: 0.0000e+00 - 6s/epoch - 21ms/step\n",
      "Epoch 2/250\n",
      "291/291 - 2s - loss: 12.6349 - mean_squared_error: 12.6349 - accuracy: 0.0000e+00 - 2s/epoch - 8ms/step\n",
      "Epoch 3/250\n",
      "291/291 - 2s - loss: 11.6052 - mean_squared_error: 11.6052 - accuracy: 0.0000e+00 - 2s/epoch - 8ms/step\n",
      "Epoch 4/250\n",
      "291/291 - 2s - loss: 9.7208 - mean_squared_error: 9.7208 - accuracy: 0.0000e+00 - 2s/epoch - 8ms/step\n",
      "Epoch 5/250\n",
      "291/291 - 2s - loss: 8.7366 - mean_squared_error: 8.7366 - accuracy: 0.0000e+00 - 2s/epoch - 8ms/step\n",
      "Epoch 6/250\n",
      "291/291 - 2s - loss: 7.9498 - mean_squared_error: 7.9498 - accuracy: 0.0000e+00 - 2s/epoch - 8ms/step\n",
      "Epoch 7/250\n",
      "291/291 - 2s - loss: 8.7711 - mean_squared_error: 8.7711 - accuracy: 0.0000e+00 - 2s/epoch - 8ms/step\n",
      "Epoch 8/250\n",
      "291/291 - 2s - loss: 7.7522 - mean_squared_error: 7.7522 - accuracy: 0.0000e+00 - 2s/epoch - 8ms/step\n",
      "Epoch 9/250\n",
      "291/291 - 2s - loss: 8.3098 - mean_squared_error: 8.3098 - accuracy: 0.0000e+00 - 2s/epoch - 8ms/step\n",
      "Epoch 10/250\n",
      "291/291 - 2s - loss: 6.6848 - mean_squared_error: 6.6848 - accuracy: 0.0000e+00 - 2s/epoch - 8ms/step\n",
      "Epoch 11/250\n",
      "291/291 - 2s - loss: 6.5553 - mean_squared_error: 6.5553 - accuracy: 0.0000e+00 - 2s/epoch - 8ms/step\n",
      "Epoch 12/250\n",
      "291/291 - 2s - loss: 6.6313 - mean_squared_error: 6.6313 - accuracy: 0.0000e+00 - 2s/epoch - 8ms/step\n",
      "Epoch 13/250\n",
      "291/291 - 2s - loss: 6.0832 - mean_squared_error: 6.0832 - accuracy: 0.0000e+00 - 2s/epoch - 8ms/step\n",
      "Epoch 14/250\n",
      "291/291 - 2s - loss: 5.5039 - mean_squared_error: 5.5039 - accuracy: 0.0000e+00 - 2s/epoch - 8ms/step\n",
      "Epoch 15/250\n",
      "291/291 - 2s - loss: 5.8516 - mean_squared_error: 5.8516 - accuracy: 0.0000e+00 - 2s/epoch - 8ms/step\n",
      "Epoch 16/250\n",
      "291/291 - 2s - loss: 5.1369 - mean_squared_error: 5.1369 - accuracy: 0.0000e+00 - 2s/epoch - 8ms/step\n",
      "Epoch 17/250\n",
      "291/291 - 2s - loss: 4.8175 - mean_squared_error: 4.8175 - accuracy: 0.0000e+00 - 2s/epoch - 8ms/step\n",
      "Epoch 18/250\n",
      "291/291 - 2s - loss: 4.0891 - mean_squared_error: 4.0891 - accuracy: 0.0000e+00 - 2s/epoch - 8ms/step\n",
      "Epoch 19/250\n",
      "291/291 - 2s - loss: 4.6636 - mean_squared_error: 4.6636 - accuracy: 0.0000e+00 - 2s/epoch - 8ms/step\n",
      "Epoch 20/250\n",
      "291/291 - 2s - loss: 5.0300 - mean_squared_error: 5.0300 - accuracy: 0.0000e+00 - 2s/epoch - 8ms/step\n",
      "Epoch 21/250\n",
      "291/291 - 2s - loss: 4.4390 - mean_squared_error: 4.4390 - accuracy: 0.0000e+00 - 2s/epoch - 7ms/step\n",
      "Epoch 22/250\n",
      "291/291 - 2s - loss: 3.8265 - mean_squared_error: 3.8265 - accuracy: 0.0000e+00 - 2s/epoch - 8ms/step\n",
      "Epoch 23/250\n",
      "291/291 - 2s - loss: 3.5938 - mean_squared_error: 3.5938 - accuracy: 0.0000e+00 - 2s/epoch - 8ms/step\n",
      "Epoch 24/250\n",
      "291/291 - 2s - loss: 3.5492 - mean_squared_error: 3.5492 - accuracy: 0.0000e+00 - 2s/epoch - 8ms/step\n",
      "Epoch 25/250\n",
      "291/291 - 2s - loss: 3.4524 - mean_squared_error: 3.4524 - accuracy: 0.0000e+00 - 2s/epoch - 8ms/step\n",
      "Epoch 26/250\n",
      "291/291 - 2s - loss: 3.7437 - mean_squared_error: 3.7437 - accuracy: 0.0000e+00 - 2s/epoch - 8ms/step\n",
      "Epoch 27/250\n",
      "291/291 - 2s - loss: 3.6660 - mean_squared_error: 3.6660 - accuracy: 0.0000e+00 - 2s/epoch - 8ms/step\n",
      "Epoch 28/250\n",
      "291/291 - 2s - loss: 3.4542 - mean_squared_error: 3.4542 - accuracy: 0.0000e+00 - 2s/epoch - 8ms/step\n",
      "Epoch 29/250\n",
      "291/291 - 2s - loss: 3.4806 - mean_squared_error: 3.4806 - accuracy: 0.0000e+00 - 2s/epoch - 8ms/step\n",
      "Epoch 30/250\n",
      "291/291 - 3s - loss: 3.5781 - mean_squared_error: 3.5781 - accuracy: 0.0000e+00 - 3s/epoch - 10ms/step\n",
      "Epoch 31/250\n",
      "291/291 - 2s - loss: 3.2477 - mean_squared_error: 3.2477 - accuracy: 0.0000e+00 - 2s/epoch - 8ms/step\n",
      "Epoch 32/250\n",
      "291/291 - 2s - loss: 3.1204 - mean_squared_error: 3.1204 - accuracy: 0.0000e+00 - 2s/epoch - 8ms/step\n",
      "Epoch 33/250\n",
      "291/291 - 2s - loss: 3.0258 - mean_squared_error: 3.0258 - accuracy: 0.0000e+00 - 2s/epoch - 8ms/step\n",
      "Epoch 34/250\n",
      "291/291 - 2s - loss: 3.2944 - mean_squared_error: 3.2944 - accuracy: 0.0000e+00 - 2s/epoch - 7ms/step\n",
      "Epoch 35/250\n",
      "291/291 - 2s - loss: 3.1049 - mean_squared_error: 3.1049 - accuracy: 0.0000e+00 - 2s/epoch - 7ms/step\n",
      "Epoch 36/250\n",
      "291/291 - 2s - loss: 3.0738 - mean_squared_error: 3.0738 - accuracy: 0.0000e+00 - 2s/epoch - 7ms/step\n",
      "Epoch 37/250\n",
      "291/291 - 2s - loss: 2.6073 - mean_squared_error: 2.6073 - accuracy: 0.0000e+00 - 2s/epoch - 8ms/step\n",
      "Epoch 38/250\n",
      "291/291 - 2s - loss: 2.8501 - mean_squared_error: 2.8501 - accuracy: 0.0000e+00 - 2s/epoch - 8ms/step\n",
      "Epoch 39/250\n",
      "291/291 - 3s - loss: 2.7374 - mean_squared_error: 2.7374 - accuracy: 0.0000e+00 - 3s/epoch - 9ms/step\n",
      "Epoch 40/250\n",
      "291/291 - 2s - loss: 2.5726 - mean_squared_error: 2.5726 - accuracy: 0.0000e+00 - 2s/epoch - 9ms/step\n",
      "Epoch 41/250\n",
      "291/291 - 2s - loss: 2.5030 - mean_squared_error: 2.5030 - accuracy: 0.0000e+00 - 2s/epoch - 8ms/step\n",
      "Epoch 42/250\n",
      "291/291 - 2s - loss: 2.4096 - mean_squared_error: 2.4096 - accuracy: 0.0000e+00 - 2s/epoch - 8ms/step\n",
      "Epoch 43/250\n",
      "291/291 - 2s - loss: 2.3847 - mean_squared_error: 2.3847 - accuracy: 0.0000e+00 - 2s/epoch - 8ms/step\n",
      "Epoch 44/250\n",
      "291/291 - 2s - loss: 2.3605 - mean_squared_error: 2.3605 - accuracy: 0.0000e+00 - 2s/epoch - 8ms/step\n",
      "Epoch 45/250\n",
      "291/291 - 2s - loss: 2.3405 - mean_squared_error: 2.3405 - accuracy: 0.0000e+00 - 2s/epoch - 8ms/step\n",
      "Epoch 46/250\n",
      "291/291 - 2s - loss: 2.1895 - mean_squared_error: 2.1895 - accuracy: 0.0000e+00 - 2s/epoch - 8ms/step\n",
      "Epoch 47/250\n",
      "291/291 - 2s - loss: 2.3079 - mean_squared_error: 2.3079 - accuracy: 0.0000e+00 - 2s/epoch - 8ms/step\n",
      "Epoch 48/250\n",
      "291/291 - 2s - loss: 1.9997 - mean_squared_error: 1.9997 - accuracy: 0.0000e+00 - 2s/epoch - 8ms/step\n",
      "Epoch 49/250\n",
      "291/291 - 2s - loss: 2.1802 - mean_squared_error: 2.1802 - accuracy: 0.0000e+00 - 2s/epoch - 7ms/step\n",
      "Epoch 50/250\n",
      "291/291 - 2s - loss: 2.1059 - mean_squared_error: 2.1059 - accuracy: 0.0000e+00 - 2s/epoch - 8ms/step\n",
      "Epoch 51/250\n",
      "291/291 - 2s - loss: 1.9967 - mean_squared_error: 1.9967 - accuracy: 0.0000e+00 - 2s/epoch - 8ms/step\n",
      "Epoch 52/250\n",
      "291/291 - 2s - loss: 2.0053 - mean_squared_error: 2.0053 - accuracy: 0.0000e+00 - 2s/epoch - 7ms/step\n",
      "Epoch 53/250\n",
      "291/291 - 2s - loss: 1.8373 - mean_squared_error: 1.8373 - accuracy: 0.0000e+00 - 2s/epoch - 8ms/step\n",
      "Epoch 54/250\n",
      "291/291 - 2s - loss: 1.9460 - mean_squared_error: 1.9460 - accuracy: 0.0000e+00 - 2s/epoch - 8ms/step\n",
      "Epoch 55/250\n",
      "291/291 - 2s - loss: 2.3633 - mean_squared_error: 2.3633 - accuracy: 0.0000e+00 - 2s/epoch - 8ms/step\n",
      "Epoch 56/250\n",
      "291/291 - 2s - loss: 1.8273 - mean_squared_error: 1.8273 - accuracy: 0.0000e+00 - 2s/epoch - 8ms/step\n",
      "Epoch 57/250\n",
      "291/291 - 2s - loss: 1.7467 - mean_squared_error: 1.7467 - accuracy: 0.0000e+00 - 2s/epoch - 8ms/step\n",
      "Epoch 58/250\n",
      "291/291 - 2s - loss: 1.7188 - mean_squared_error: 1.7188 - accuracy: 0.0000e+00 - 2s/epoch - 8ms/step\n",
      "Epoch 59/250\n",
      "291/291 - 2s - loss: 1.7899 - mean_squared_error: 1.7899 - accuracy: 0.0000e+00 - 2s/epoch - 8ms/step\n",
      "Epoch 60/250\n",
      "291/291 - 2s - loss: 1.8073 - mean_squared_error: 1.8073 - accuracy: 0.0000e+00 - 2s/epoch - 8ms/step\n",
      "Epoch 61/250\n",
      "291/291 - 2s - loss: 1.4803 - mean_squared_error: 1.4803 - accuracy: 0.0000e+00 - 2s/epoch - 8ms/step\n",
      "Epoch 62/250\n",
      "291/291 - 2s - loss: 1.4669 - mean_squared_error: 1.4669 - accuracy: 0.0000e+00 - 2s/epoch - 8ms/step\n",
      "Epoch 63/250\n",
      "291/291 - 2s - loss: 1.7567 - mean_squared_error: 1.7567 - accuracy: 0.0000e+00 - 2s/epoch - 7ms/step\n",
      "Epoch 64/250\n",
      "291/291 - 2s - loss: 1.5621 - mean_squared_error: 1.5621 - accuracy: 0.0000e+00 - 2s/epoch - 8ms/step\n",
      "Epoch 65/250\n",
      "291/291 - 2s - loss: 1.4516 - mean_squared_error: 1.4516 - accuracy: 0.0000e+00 - 2s/epoch - 8ms/step\n",
      "Epoch 66/250\n",
      "291/291 - 2s - loss: 1.3230 - mean_squared_error: 1.3230 - accuracy: 0.0000e+00 - 2s/epoch - 7ms/step\n",
      "Epoch 67/250\n",
      "291/291 - 2s - loss: 1.5341 - mean_squared_error: 1.5341 - accuracy: 0.0000e+00 - 2s/epoch - 8ms/step\n",
      "Epoch 68/250\n",
      "291/291 - 2s - loss: 1.2430 - mean_squared_error: 1.2430 - accuracy: 0.0000e+00 - 2s/epoch - 7ms/step\n",
      "Epoch 69/250\n",
      "291/291 - 2s - loss: 1.1493 - mean_squared_error: 1.1493 - accuracy: 0.0000e+00 - 2s/epoch - 7ms/step\n",
      "Epoch 70/250\n",
      "291/291 - 2s - loss: 1.2052 - mean_squared_error: 1.2052 - accuracy: 0.0000e+00 - 2s/epoch - 8ms/step\n",
      "Epoch 71/250\n",
      "291/291 - 2s - loss: 1.1438 - mean_squared_error: 1.1438 - accuracy: 0.0000e+00 - 2s/epoch - 8ms/step\n",
      "Epoch 72/250\n",
      "291/291 - 2s - loss: 0.9246 - mean_squared_error: 0.9246 - accuracy: 0.0000e+00 - 2s/epoch - 7ms/step\n",
      "Epoch 73/250\n",
      "291/291 - 2s - loss: 1.2640 - mean_squared_error: 1.2640 - accuracy: 0.0000e+00 - 2s/epoch - 7ms/step\n",
      "Epoch 74/250\n",
      "291/291 - 2s - loss: 1.0574 - mean_squared_error: 1.0574 - accuracy: 0.0000e+00 - 2s/epoch - 8ms/step\n",
      "Epoch 75/250\n",
      "291/291 - 2s - loss: 1.1834 - mean_squared_error: 1.1834 - accuracy: 0.0000e+00 - 2s/epoch - 8ms/step\n",
      "Epoch 76/250\n",
      "291/291 - 2s - loss: 1.1461 - mean_squared_error: 1.1461 - accuracy: 0.0000e+00 - 2s/epoch - 8ms/step\n",
      "Epoch 77/250\n",
      "291/291 - 2s - loss: 0.9473 - mean_squared_error: 0.9473 - accuracy: 0.0000e+00 - 2s/epoch - 8ms/step\n",
      "Epoch 78/250\n",
      "291/291 - 2s - loss: 0.7752 - mean_squared_error: 0.7752 - accuracy: 0.0000e+00 - 2s/epoch - 8ms/step\n",
      "Epoch 79/250\n",
      "291/291 - 2s - loss: 1.1392 - mean_squared_error: 1.1392 - accuracy: 0.0000e+00 - 2s/epoch - 8ms/step\n",
      "Epoch 80/250\n",
      "291/291 - 2s - loss: 0.8257 - mean_squared_error: 0.8257 - accuracy: 0.0000e+00 - 2s/epoch - 8ms/step\n",
      "Epoch 81/250\n",
      "291/291 - 2s - loss: 0.8970 - mean_squared_error: 0.8970 - accuracy: 0.0000e+00 - 2s/epoch - 8ms/step\n",
      "Epoch 82/250\n",
      "291/291 - 2s - loss: 0.9053 - mean_squared_error: 0.9053 - accuracy: 0.0000e+00 - 2s/epoch - 8ms/step\n",
      "Epoch 83/250\n",
      "291/291 - 2s - loss: 0.9547 - mean_squared_error: 0.9547 - accuracy: 0.0000e+00 - 2s/epoch - 8ms/step\n",
      "Epoch 84/250\n",
      "291/291 - 2s - loss: 0.6475 - mean_squared_error: 0.6475 - accuracy: 0.0000e+00 - 2s/epoch - 7ms/step\n",
      "Epoch 85/250\n",
      "291/291 - 2s - loss: 0.6803 - mean_squared_error: 0.6803 - accuracy: 0.0000e+00 - 2s/epoch - 8ms/step\n",
      "Epoch 86/250\n",
      "291/291 - 2s - loss: 0.6668 - mean_squared_error: 0.6668 - accuracy: 0.0000e+00 - 2s/epoch - 8ms/step\n",
      "Epoch 87/250\n",
      "291/291 - 2s - loss: 1.4554 - mean_squared_error: 1.4554 - accuracy: 0.0000e+00 - 2s/epoch - 8ms/step\n",
      "Epoch 88/250\n",
      "291/291 - 2s - loss: 0.6996 - mean_squared_error: 0.6996 - accuracy: 0.0000e+00 - 2s/epoch - 8ms/step\n",
      "Epoch 89/250\n",
      "291/291 - 2s - loss: 0.5531 - mean_squared_error: 0.5531 - accuracy: 0.0000e+00 - 2s/epoch - 8ms/step\n",
      "Epoch 90/250\n",
      "291/291 - 2s - loss: 0.9397 - mean_squared_error: 0.9397 - accuracy: 0.0000e+00 - 2s/epoch - 8ms/step\n",
      "Epoch 91/250\n",
      "291/291 - 2s - loss: 0.5986 - mean_squared_error: 0.5986 - accuracy: 0.0000e+00 - 2s/epoch - 8ms/step\n",
      "Epoch 92/250\n",
      "291/291 - 2s - loss: 0.8792 - mean_squared_error: 0.8792 - accuracy: 0.0000e+00 - 2s/epoch - 8ms/step\n",
      "Epoch 93/250\n",
      "291/291 - 2s - loss: 0.5491 - mean_squared_error: 0.5491 - accuracy: 0.0000e+00 - 2s/epoch - 8ms/step\n",
      "Epoch 94/250\n",
      "291/291 - 2s - loss: 0.7159 - mean_squared_error: 0.7159 - accuracy: 0.0000e+00 - 2s/epoch - 8ms/step\n",
      "Epoch 95/250\n",
      "291/291 - 2s - loss: 0.5280 - mean_squared_error: 0.5280 - accuracy: 0.0000e+00 - 2s/epoch - 8ms/step\n",
      "Epoch 96/250\n",
      "291/291 - 2s - loss: 0.7951 - mean_squared_error: 0.7951 - accuracy: 0.0000e+00 - 2s/epoch - 7ms/step\n",
      "Epoch 97/250\n",
      "291/291 - 2s - loss: 0.7038 - mean_squared_error: 0.7038 - accuracy: 0.0000e+00 - 2s/epoch - 7ms/step\n",
      "Epoch 98/250\n",
      "291/291 - 2s - loss: 0.4785 - mean_squared_error: 0.4785 - accuracy: 0.0000e+00 - 2s/epoch - 8ms/step\n",
      "Epoch 99/250\n",
      "291/291 - 2s - loss: 0.6163 - mean_squared_error: 0.6163 - accuracy: 0.0000e+00 - 2s/epoch - 8ms/step\n",
      "Epoch 100/250\n",
      "291/291 - 3s - loss: 0.5202 - mean_squared_error: 0.5202 - accuracy: 0.0000e+00 - val_loss: 0.4526 - val_mean_squared_error: 0.4526 - val_accuracy: 0.0000e+00 - 3s/epoch - 11ms/step\n",
      "Epoch 101/250\n",
      "291/291 - 2s - loss: 0.7707 - mean_squared_error: 0.7707 - accuracy: 0.0000e+00 - 2s/epoch - 8ms/step\n",
      "Epoch 102/250\n",
      "291/291 - 2s - loss: 1.1567 - mean_squared_error: 1.1567 - accuracy: 0.0000e+00 - 2s/epoch - 8ms/step\n",
      "Epoch 103/250\n",
      "291/291 - 2s - loss: 0.4386 - mean_squared_error: 0.4386 - accuracy: 0.0000e+00 - 2s/epoch - 8ms/step\n",
      "Epoch 104/250\n",
      "291/291 - 2s - loss: 0.5226 - mean_squared_error: 0.5226 - accuracy: 0.0000e+00 - 2s/epoch - 8ms/step\n",
      "Epoch 105/250\n",
      "291/291 - 2s - loss: 0.4287 - mean_squared_error: 0.4287 - accuracy: 0.0000e+00 - 2s/epoch - 8ms/step\n",
      "Epoch 106/250\n",
      "291/291 - 2s - loss: 0.9136 - mean_squared_error: 0.9136 - accuracy: 0.0000e+00 - 2s/epoch - 8ms/step\n",
      "Epoch 107/250\n",
      "291/291 - 2s - loss: 1.2266 - mean_squared_error: 1.2266 - accuracy: 0.0000e+00 - 2s/epoch - 8ms/step\n",
      "Epoch 108/250\n",
      "291/291 - 2s - loss: 0.4355 - mean_squared_error: 0.4355 - accuracy: 0.0000e+00 - 2s/epoch - 8ms/step\n",
      "Epoch 109/250\n",
      "291/291 - 2s - loss: 0.3915 - mean_squared_error: 0.3915 - accuracy: 0.0000e+00 - 2s/epoch - 8ms/step\n",
      "Epoch 110/250\n",
      "291/291 - 2s - loss: 0.4497 - mean_squared_error: 0.4497 - accuracy: 0.0000e+00 - 2s/epoch - 8ms/step\n",
      "Epoch 111/250\n",
      "291/291 - 2s - loss: 0.3662 - mean_squared_error: 0.3662 - accuracy: 0.0000e+00 - 2s/epoch - 8ms/step\n",
      "Epoch 112/250\n",
      "291/291 - 2s - loss: 0.4255 - mean_squared_error: 0.4255 - accuracy: 0.0000e+00 - 2s/epoch - 8ms/step\n",
      "Epoch 113/250\n",
      "291/291 - 2s - loss: 0.4303 - mean_squared_error: 0.4303 - accuracy: 0.0000e+00 - 2s/epoch - 8ms/step\n",
      "Epoch 114/250\n",
      "291/291 - 2s - loss: 0.4279 - mean_squared_error: 0.4279 - accuracy: 0.0000e+00 - 2s/epoch - 8ms/step\n",
      "Epoch 115/250\n",
      "291/291 - 2s - loss: 0.7423 - mean_squared_error: 0.7423 - accuracy: 0.0000e+00 - 2s/epoch - 8ms/step\n",
      "Epoch 116/250\n",
      "291/291 - 2s - loss: 1.8788 - mean_squared_error: 1.8788 - accuracy: 0.0000e+00 - 2s/epoch - 8ms/step\n",
      "Epoch 117/250\n",
      "291/291 - 2s - loss: 0.4325 - mean_squared_error: 0.4325 - accuracy: 0.0000e+00 - 2s/epoch - 8ms/step\n",
      "Epoch 118/250\n",
      "291/291 - 2s - loss: 0.4933 - mean_squared_error: 0.4933 - accuracy: 0.0000e+00 - 2s/epoch - 8ms/step\n",
      "Epoch 119/250\n",
      "291/291 - 2s - loss: 0.3550 - mean_squared_error: 0.3550 - accuracy: 0.0000e+00 - 2s/epoch - 8ms/step\n",
      "Epoch 120/250\n",
      "291/291 - 2s - loss: 0.4332 - mean_squared_error: 0.4332 - accuracy: 0.0000e+00 - 2s/epoch - 8ms/step\n",
      "Epoch 121/250\n",
      "291/291 - 2s - loss: 0.3845 - mean_squared_error: 0.3845 - accuracy: 0.0000e+00 - 2s/epoch - 8ms/step\n",
      "Epoch 122/250\n",
      "291/291 - 2s - loss: 0.7877 - mean_squared_error: 0.7877 - accuracy: 0.0000e+00 - 2s/epoch - 8ms/step\n",
      "Epoch 123/250\n",
      "291/291 - 2s - loss: 0.5208 - mean_squared_error: 0.5208 - accuracy: 0.0000e+00 - 2s/epoch - 8ms/step\n",
      "Epoch 124/250\n",
      "291/291 - 2s - loss: 0.3114 - mean_squared_error: 0.3114 - accuracy: 0.0000e+00 - 2s/epoch - 8ms/step\n",
      "Epoch 125/250\n",
      "291/291 - 2s - loss: 0.4454 - mean_squared_error: 0.4454 - accuracy: 0.0000e+00 - 2s/epoch - 8ms/step\n",
      "Epoch 126/250\n",
      "291/291 - 2s - loss: 0.6184 - mean_squared_error: 0.6184 - accuracy: 0.0000e+00 - 2s/epoch - 8ms/step\n",
      "Epoch 127/250\n",
      "291/291 - 2s - loss: 0.4279 - mean_squared_error: 0.4279 - accuracy: 0.0000e+00 - 2s/epoch - 8ms/step\n",
      "Epoch 128/250\n",
      "291/291 - 2s - loss: 0.3662 - mean_squared_error: 0.3662 - accuracy: 0.0000e+00 - 2s/epoch - 8ms/step\n",
      "Epoch 129/250\n",
      "291/291 - 2s - loss: 0.4082 - mean_squared_error: 0.4082 - accuracy: 0.0000e+00 - 2s/epoch - 8ms/step\n",
      "Epoch 130/250\n",
      "291/291 - 2s - loss: 0.6284 - mean_squared_error: 0.6284 - accuracy: 0.0000e+00 - 2s/epoch - 8ms/step\n",
      "Epoch 131/250\n",
      "291/291 - 2s - loss: 0.4890 - mean_squared_error: 0.4890 - accuracy: 0.0000e+00 - 2s/epoch - 8ms/step\n",
      "Epoch 132/250\n",
      "291/291 - 2s - loss: 0.3542 - mean_squared_error: 0.3542 - accuracy: 0.0000e+00 - 2s/epoch - 8ms/step\n",
      "Epoch 133/250\n",
      "291/291 - 2s - loss: 0.4201 - mean_squared_error: 0.4201 - accuracy: 0.0000e+00 - 2s/epoch - 8ms/step\n",
      "Epoch 134/250\n",
      "291/291 - 2s - loss: 0.2892 - mean_squared_error: 0.2892 - accuracy: 0.0000e+00 - 2s/epoch - 8ms/step\n",
      "Epoch 135/250\n",
      "291/291 - 2s - loss: 0.3466 - mean_squared_error: 0.3466 - accuracy: 0.0000e+00 - 2s/epoch - 8ms/step\n",
      "Epoch 136/250\n",
      "291/291 - 2s - loss: 0.6778 - mean_squared_error: 0.6778 - accuracy: 0.0000e+00 - 2s/epoch - 8ms/step\n",
      "Epoch 137/250\n",
      "291/291 - 2s - loss: 0.8462 - mean_squared_error: 0.8462 - accuracy: 0.0000e+00 - 2s/epoch - 8ms/step\n",
      "Epoch 138/250\n",
      "291/291 - 2s - loss: 0.3280 - mean_squared_error: 0.3280 - accuracy: 0.0000e+00 - 2s/epoch - 8ms/step\n",
      "Epoch 139/250\n",
      "291/291 - 2s - loss: 0.3528 - mean_squared_error: 0.3528 - accuracy: 0.0000e+00 - 2s/epoch - 8ms/step\n",
      "Epoch 140/250\n",
      "291/291 - 2s - loss: 0.3925 - mean_squared_error: 0.3925 - accuracy: 0.0000e+00 - 2s/epoch - 8ms/step\n",
      "Epoch 141/250\n",
      "291/291 - 2s - loss: 0.3173 - mean_squared_error: 0.3173 - accuracy: 0.0000e+00 - 2s/epoch - 8ms/step\n",
      "Epoch 142/250\n",
      "291/291 - 2s - loss: 0.3085 - mean_squared_error: 0.3085 - accuracy: 0.0000e+00 - 2s/epoch - 8ms/step\n",
      "Epoch 143/250\n",
      "291/291 - 2s - loss: 0.2414 - mean_squared_error: 0.2414 - accuracy: 0.0000e+00 - 2s/epoch - 8ms/step\n",
      "Epoch 144/250\n",
      "291/291 - 2s - loss: 0.6548 - mean_squared_error: 0.6548 - accuracy: 0.0000e+00 - 2s/epoch - 8ms/step\n",
      "Epoch 145/250\n",
      "291/291 - 2s - loss: 0.3560 - mean_squared_error: 0.3560 - accuracy: 0.0000e+00 - 2s/epoch - 8ms/step\n",
      "Epoch 146/250\n",
      "291/291 - 2s - loss: 0.3227 - mean_squared_error: 0.3227 - accuracy: 0.0000e+00 - 2s/epoch - 8ms/step\n",
      "Epoch 147/250\n",
      "291/291 - 2s - loss: 0.2710 - mean_squared_error: 0.2710 - accuracy: 0.0000e+00 - 2s/epoch - 8ms/step\n",
      "Epoch 148/250\n",
      "291/291 - 2s - loss: 0.3356 - mean_squared_error: 0.3356 - accuracy: 0.0000e+00 - 2s/epoch - 8ms/step\n",
      "Epoch 149/250\n",
      "291/291 - 2s - loss: 0.6162 - mean_squared_error: 0.6162 - accuracy: 0.0000e+00 - 2s/epoch - 8ms/step\n",
      "Epoch 150/250\n",
      "291/291 - 2s - loss: 0.4088 - mean_squared_error: 0.4088 - accuracy: 0.0000e+00 - 2s/epoch - 8ms/step\n",
      "Epoch 151/250\n",
      "291/291 - 2s - loss: 0.2964 - mean_squared_error: 0.2964 - accuracy: 0.0000e+00 - 2s/epoch - 8ms/step\n",
      "Epoch 152/250\n",
      "291/291 - 2s - loss: 0.3531 - mean_squared_error: 0.3531 - accuracy: 0.0000e+00 - 2s/epoch - 7ms/step\n",
      "Epoch 153/250\n",
      "291/291 - 2s - loss: 0.3344 - mean_squared_error: 0.3344 - accuracy: 0.0000e+00 - 2s/epoch - 8ms/step\n",
      "Epoch 154/250\n",
      "291/291 - 2s - loss: 0.2488 - mean_squared_error: 0.2488 - accuracy: 0.0000e+00 - 2s/epoch - 8ms/step\n",
      "Epoch 155/250\n",
      "291/291 - 2s - loss: 0.5253 - mean_squared_error: 0.5253 - accuracy: 0.0000e+00 - 2s/epoch - 8ms/step\n",
      "Epoch 156/250\n",
      "291/291 - 2s - loss: 0.3100 - mean_squared_error: 0.3100 - accuracy: 0.0000e+00 - 2s/epoch - 7ms/step\n",
      "Epoch 157/250\n",
      "291/291 - 2s - loss: 0.4048 - mean_squared_error: 0.4048 - accuracy: 0.0000e+00 - 2s/epoch - 7ms/step\n",
      "Epoch 158/250\n",
      "291/291 - 2s - loss: 0.2795 - mean_squared_error: 0.2795 - accuracy: 0.0000e+00 - 2s/epoch - 8ms/step\n",
      "Epoch 159/250\n",
      "291/291 - 2s - loss: 0.2878 - mean_squared_error: 0.2878 - accuracy: 0.0000e+00 - 2s/epoch - 8ms/step\n",
      "Epoch 160/250\n",
      "291/291 - 2s - loss: 0.2482 - mean_squared_error: 0.2482 - accuracy: 0.0000e+00 - 2s/epoch - 8ms/step\n",
      "Epoch 161/250\n",
      "291/291 - 2s - loss: 0.5747 - mean_squared_error: 0.5747 - accuracy: 0.0000e+00 - 2s/epoch - 8ms/step\n",
      "Epoch 162/250\n",
      "291/291 - 2s - loss: 0.8883 - mean_squared_error: 0.8883 - accuracy: 0.0000e+00 - 2s/epoch - 8ms/step\n",
      "Epoch 163/250\n",
      "291/291 - 2s - loss: 0.2370 - mean_squared_error: 0.2370 - accuracy: 0.0000e+00 - 2s/epoch - 8ms/step\n",
      "Epoch 164/250\n",
      "291/291 - 2s - loss: 0.2191 - mean_squared_error: 0.2191 - accuracy: 0.0000e+00 - 2s/epoch - 7ms/step\n",
      "Epoch 165/250\n",
      "291/291 - 2s - loss: 0.3913 - mean_squared_error: 0.3913 - accuracy: 0.0000e+00 - 2s/epoch - 9ms/step\n",
      "Epoch 166/250\n",
      "291/291 - 2s - loss: 0.2760 - mean_squared_error: 0.2760 - accuracy: 0.0000e+00 - 2s/epoch - 8ms/step\n",
      "Epoch 167/250\n",
      "291/291 - 2s - loss: 0.2211 - mean_squared_error: 0.2211 - accuracy: 0.0000e+00 - 2s/epoch - 8ms/step\n",
      "Epoch 168/250\n",
      "291/291 - 2s - loss: 0.2539 - mean_squared_error: 0.2539 - accuracy: 0.0000e+00 - 2s/epoch - 8ms/step\n",
      "Epoch 169/250\n",
      "291/291 - 2s - loss: 0.2488 - mean_squared_error: 0.2488 - accuracy: 0.0000e+00 - 2s/epoch - 7ms/step\n",
      "Epoch 170/250\n",
      "291/291 - 2s - loss: 0.9298 - mean_squared_error: 0.9298 - accuracy: 0.0000e+00 - 2s/epoch - 7ms/step\n",
      "Epoch 171/250\n",
      "291/291 - 2s - loss: 0.3411 - mean_squared_error: 0.3411 - accuracy: 0.0000e+00 - 2s/epoch - 8ms/step\n",
      "Epoch 172/250\n",
      "291/291 - 2s - loss: 0.3986 - mean_squared_error: 0.3986 - accuracy: 0.0000e+00 - 2s/epoch - 7ms/step\n",
      "Epoch 173/250\n",
      "291/291 - 2s - loss: 0.2343 - mean_squared_error: 0.2343 - accuracy: 0.0000e+00 - 2s/epoch - 7ms/step\n",
      "Epoch 174/250\n",
      "291/291 - 2s - loss: 0.1941 - mean_squared_error: 0.1941 - accuracy: 0.0000e+00 - 2s/epoch - 7ms/step\n",
      "Epoch 175/250\n",
      "291/291 - 2s - loss: 0.2138 - mean_squared_error: 0.2138 - accuracy: 0.0000e+00 - 2s/epoch - 7ms/step\n",
      "Epoch 176/250\n",
      "291/291 - 2s - loss: 0.2120 - mean_squared_error: 0.2120 - accuracy: 0.0000e+00 - 2s/epoch - 7ms/step\n",
      "Epoch 177/250\n",
      "291/291 - 2s - loss: 0.3058 - mean_squared_error: 0.3058 - accuracy: 0.0000e+00 - 2s/epoch - 7ms/step\n",
      "Epoch 178/250\n",
      "291/291 - 2s - loss: 0.2936 - mean_squared_error: 0.2936 - accuracy: 0.0000e+00 - 2s/epoch - 8ms/step\n",
      "Epoch 179/250\n",
      "291/291 - 2s - loss: 0.4544 - mean_squared_error: 0.4544 - accuracy: 0.0000e+00 - 2s/epoch - 7ms/step\n",
      "Epoch 180/250\n",
      "291/291 - 2s - loss: 0.2631 - mean_squared_error: 0.2631 - accuracy: 0.0000e+00 - 2s/epoch - 7ms/step\n",
      "Epoch 181/250\n",
      "291/291 - 2s - loss: 0.2003 - mean_squared_error: 0.2003 - accuracy: 0.0000e+00 - 2s/epoch - 8ms/step\n",
      "Epoch 182/250\n",
      "291/291 - 2s - loss: 0.2366 - mean_squared_error: 0.2366 - accuracy: 0.0000e+00 - 2s/epoch - 9ms/step\n",
      "Epoch 183/250\n",
      "291/291 - 2s - loss: 0.8289 - mean_squared_error: 0.8289 - accuracy: 0.0000e+00 - 2s/epoch - 8ms/step\n",
      "Epoch 184/250\n",
      "291/291 - 2s - loss: 1.0697 - mean_squared_error: 1.0697 - accuracy: 0.0000e+00 - 2s/epoch - 8ms/step\n",
      "Epoch 185/250\n",
      "291/291 - 2s - loss: 0.2655 - mean_squared_error: 0.2655 - accuracy: 0.0000e+00 - 2s/epoch - 7ms/step\n",
      "Epoch 186/250\n",
      "291/291 - 2s - loss: 0.1922 - mean_squared_error: 0.1922 - accuracy: 0.0000e+00 - 2s/epoch - 7ms/step\n",
      "Epoch 187/250\n",
      "291/291 - 2s - loss: 0.1676 - mean_squared_error: 0.1676 - accuracy: 0.0000e+00 - 2s/epoch - 7ms/step\n",
      "Epoch 188/250\n",
      "291/291 - 2s - loss: 0.1622 - mean_squared_error: 0.1622 - accuracy: 0.0000e+00 - 2s/epoch - 7ms/step\n",
      "Epoch 189/250\n",
      "291/291 - 2s - loss: 0.2354 - mean_squared_error: 0.2354 - accuracy: 0.0000e+00 - 2s/epoch - 7ms/step\n",
      "Epoch 190/250\n",
      "291/291 - 2s - loss: 0.2115 - mean_squared_error: 0.2115 - accuracy: 0.0000e+00 - 2s/epoch - 7ms/step\n",
      "Epoch 191/250\n",
      "291/291 - 2s - loss: 0.2410 - mean_squared_error: 0.2410 - accuracy: 0.0000e+00 - 2s/epoch - 8ms/step\n",
      "Epoch 192/250\n",
      "291/291 - 2s - loss: 0.3745 - mean_squared_error: 0.3745 - accuracy: 0.0000e+00 - 2s/epoch - 8ms/step\n",
      "Epoch 193/250\n",
      "291/291 - 2s - loss: 2.0941 - mean_squared_error: 2.0941 - accuracy: 0.0000e+00 - 2s/epoch - 8ms/step\n",
      "Epoch 194/250\n",
      "291/291 - 2s - loss: 0.3745 - mean_squared_error: 0.3745 - accuracy: 0.0000e+00 - 2s/epoch - 8ms/step\n",
      "Epoch 195/250\n",
      "291/291 - 2s - loss: 0.1843 - mean_squared_error: 0.1843 - accuracy: 0.0000e+00 - 2s/epoch - 8ms/step\n",
      "Epoch 196/250\n",
      "291/291 - 2s - loss: 0.1625 - mean_squared_error: 0.1625 - accuracy: 0.0000e+00 - 2s/epoch - 8ms/step\n",
      "Epoch 197/250\n",
      "291/291 - 2s - loss: 0.1987 - mean_squared_error: 0.1987 - accuracy: 0.0000e+00 - 2s/epoch - 8ms/step\n",
      "Epoch 198/250\n",
      "291/291 - 2s - loss: 0.2403 - mean_squared_error: 0.2403 - accuracy: 0.0000e+00 - 2s/epoch - 8ms/step\n",
      "Epoch 199/250\n",
      "291/291 - 2s - loss: 0.1854 - mean_squared_error: 0.1854 - accuracy: 0.0000e+00 - 2s/epoch - 8ms/step\n",
      "Epoch 200/250\n",
      "291/291 - 3s - loss: 0.2532 - mean_squared_error: 0.2532 - accuracy: 0.0000e+00 - val_loss: 0.5005 - val_mean_squared_error: 0.5005 - val_accuracy: 0.0000e+00 - 3s/epoch - 9ms/step\n",
      "Epoch 201/250\n",
      "291/291 - 2s - loss: 0.2431 - mean_squared_error: 0.2431 - accuracy: 0.0000e+00 - 2s/epoch - 8ms/step\n",
      "Epoch 202/250\n",
      "291/291 - 2s - loss: 0.3466 - mean_squared_error: 0.3466 - accuracy: 0.0000e+00 - 2s/epoch - 8ms/step\n",
      "Epoch 203/250\n",
      "291/291 - 2s - loss: 0.3217 - mean_squared_error: 0.3217 - accuracy: 0.0000e+00 - 2s/epoch - 8ms/step\n",
      "Epoch 204/250\n",
      "291/291 - 2s - loss: 0.2319 - mean_squared_error: 0.2319 - accuracy: 0.0000e+00 - 2s/epoch - 8ms/step\n",
      "Epoch 205/250\n",
      "291/291 - 2s - loss: 0.3645 - mean_squared_error: 0.3645 - accuracy: 0.0000e+00 - 2s/epoch - 8ms/step\n",
      "Epoch 206/250\n",
      "291/291 - 2s - loss: 0.5799 - mean_squared_error: 0.5799 - accuracy: 0.0000e+00 - 2s/epoch - 7ms/step\n",
      "Epoch 207/250\n",
      "291/291 - 2s - loss: 0.2047 - mean_squared_error: 0.2047 - accuracy: 0.0000e+00 - 2s/epoch - 7ms/step\n",
      "Epoch 208/250\n",
      "291/291 - 2s - loss: 0.2219 - mean_squared_error: 0.2219 - accuracy: 0.0000e+00 - 2s/epoch - 8ms/step\n",
      "Epoch 209/250\n",
      "291/291 - 2s - loss: 0.1891 - mean_squared_error: 0.1891 - accuracy: 0.0000e+00 - 2s/epoch - 8ms/step\n",
      "Epoch 210/250\n",
      "291/291 - 2s - loss: 0.2029 - mean_squared_error: 0.2029 - accuracy: 0.0000e+00 - 2s/epoch - 8ms/step\n",
      "Epoch 211/250\n",
      "291/291 - 2s - loss: 0.3582 - mean_squared_error: 0.3582 - accuracy: 0.0000e+00 - 2s/epoch - 8ms/step\n",
      "Epoch 212/250\n",
      "291/291 - 2s - loss: 0.1922 - mean_squared_error: 0.1922 - accuracy: 0.0000e+00 - 2s/epoch - 8ms/step\n",
      "Epoch 213/250\n",
      "291/291 - 2s - loss: 0.3383 - mean_squared_error: 0.3383 - accuracy: 0.0000e+00 - 2s/epoch - 8ms/step\n",
      "Epoch 214/250\n",
      "291/291 - 2s - loss: 0.1923 - mean_squared_error: 0.1923 - accuracy: 0.0000e+00 - 2s/epoch - 8ms/step\n",
      "Epoch 215/250\n",
      "291/291 - 2s - loss: 0.1840 - mean_squared_error: 0.1840 - accuracy: 0.0000e+00 - 2s/epoch - 7ms/step\n",
      "Epoch 216/250\n",
      "291/291 - 2s - loss: 0.4796 - mean_squared_error: 0.4796 - accuracy: 0.0000e+00 - 2s/epoch - 7ms/step\n",
      "Epoch 217/250\n",
      "291/291 - 2s - loss: 0.3523 - mean_squared_error: 0.3523 - accuracy: 0.0000e+00 - 2s/epoch - 8ms/step\n",
      "Epoch 218/250\n",
      "291/291 - 2s - loss: 0.1988 - mean_squared_error: 0.1988 - accuracy: 0.0000e+00 - 2s/epoch - 8ms/step\n",
      "Epoch 219/250\n",
      "291/291 - 2s - loss: 0.1714 - mean_squared_error: 0.1714 - accuracy: 0.0000e+00 - 2s/epoch - 8ms/step\n",
      "Epoch 220/250\n",
      "291/291 - 2s - loss: 0.1921 - mean_squared_error: 0.1921 - accuracy: 0.0000e+00 - 2s/epoch - 7ms/step\n",
      "Epoch 221/250\n",
      "291/291 - 2s - loss: 0.1913 - mean_squared_error: 0.1913 - accuracy: 0.0000e+00 - 2s/epoch - 7ms/step\n",
      "Epoch 222/250\n",
      "291/291 - 2s - loss: 0.2739 - mean_squared_error: 0.2739 - accuracy: 0.0000e+00 - 2s/epoch - 7ms/step\n",
      "Epoch 223/250\n",
      "291/291 - 2s - loss: 0.4889 - mean_squared_error: 0.4889 - accuracy: 0.0000e+00 - 2s/epoch - 7ms/step\n",
      "Epoch 224/250\n",
      "291/291 - 2s - loss: 0.1966 - mean_squared_error: 0.1966 - accuracy: 0.0000e+00 - 2s/epoch - 7ms/step\n",
      "Epoch 225/250\n",
      "291/291 - 2s - loss: 0.1753 - mean_squared_error: 0.1753 - accuracy: 0.0000e+00 - 2s/epoch - 8ms/step\n",
      "Epoch 226/250\n",
      "291/291 - 2s - loss: 0.2013 - mean_squared_error: 0.2013 - accuracy: 0.0000e+00 - 2s/epoch - 8ms/step\n",
      "Epoch 227/250\n",
      "291/291 - 2s - loss: 0.2030 - mean_squared_error: 0.2030 - accuracy: 0.0000e+00 - 2s/epoch - 7ms/step\n",
      "Epoch 228/250\n",
      "291/291 - 2s - loss: 0.3361 - mean_squared_error: 0.3361 - accuracy: 0.0000e+00 - 2s/epoch - 7ms/step\n",
      "Epoch 229/250\n",
      "291/291 - 2s - loss: 0.2199 - mean_squared_error: 0.2199 - accuracy: 0.0000e+00 - 2s/epoch - 7ms/step\n",
      "Epoch 230/250\n",
      "291/291 - 2s - loss: 0.1627 - mean_squared_error: 0.1627 - accuracy: 0.0000e+00 - 2s/epoch - 7ms/step\n",
      "Epoch 231/250\n",
      "291/291 - 2s - loss: 0.2849 - mean_squared_error: 0.2849 - accuracy: 0.0000e+00 - 2s/epoch - 8ms/step\n",
      "Epoch 232/250\n",
      "291/291 - 2s - loss: 0.2382 - mean_squared_error: 0.2382 - accuracy: 0.0000e+00 - 2s/epoch - 8ms/step\n",
      "Epoch 233/250\n",
      "291/291 - 2s - loss: 0.2533 - mean_squared_error: 0.2533 - accuracy: 0.0000e+00 - 2s/epoch - 8ms/step\n",
      "Epoch 234/250\n",
      "291/291 - 2s - loss: 0.2400 - mean_squared_error: 0.2400 - accuracy: 0.0000e+00 - 2s/epoch - 7ms/step\n",
      "Epoch 235/250\n",
      "291/291 - 2s - loss: 0.1786 - mean_squared_error: 0.1786 - accuracy: 0.0000e+00 - 2s/epoch - 8ms/step\n",
      "Epoch 236/250\n",
      "291/291 - 2s - loss: 0.5317 - mean_squared_error: 0.5317 - accuracy: 0.0000e+00 - 2s/epoch - 7ms/step\n",
      "Epoch 237/250\n",
      "291/291 - 2s - loss: 0.9963 - mean_squared_error: 0.9963 - accuracy: 0.0000e+00 - 2s/epoch - 7ms/step\n",
      "Epoch 238/250\n",
      "291/291 - 2s - loss: 0.1797 - mean_squared_error: 0.1797 - accuracy: 0.0000e+00 - 2s/epoch - 8ms/step\n",
      "Epoch 239/250\n",
      "291/291 - 2s - loss: 0.1615 - mean_squared_error: 0.1615 - accuracy: 0.0000e+00 - 2s/epoch - 7ms/step\n",
      "Epoch 240/250\n",
      "291/291 - 2s - loss: 0.1578 - mean_squared_error: 0.1578 - accuracy: 0.0000e+00 - 2s/epoch - 8ms/step\n",
      "Epoch 241/250\n",
      "291/291 - 3s - loss: 0.1770 - mean_squared_error: 0.1770 - accuracy: 0.0000e+00 - 3s/epoch - 9ms/step\n",
      "Epoch 242/250\n",
      "291/291 - 3s - loss: 0.1738 - mean_squared_error: 0.1738 - accuracy: 0.0000e+00 - 3s/epoch - 9ms/step\n",
      "Epoch 243/250\n",
      "291/291 - 2s - loss: 0.1501 - mean_squared_error: 0.1501 - accuracy: 0.0000e+00 - 2s/epoch - 8ms/step\n",
      "Epoch 244/250\n",
      "291/291 - 3s - loss: 0.2458 - mean_squared_error: 0.2458 - accuracy: 0.0000e+00 - 3s/epoch - 10ms/step\n",
      "Epoch 245/250\n",
      "291/291 - 3s - loss: 0.1640 - mean_squared_error: 0.1640 - accuracy: 0.0000e+00 - 3s/epoch - 9ms/step\n",
      "Epoch 246/250\n",
      "291/291 - 2s - loss: 0.1835 - mean_squared_error: 0.1835 - accuracy: 0.0000e+00 - 2s/epoch - 8ms/step\n",
      "Epoch 247/250\n",
      "291/291 - 2s - loss: 0.2801 - mean_squared_error: 0.2801 - accuracy: 0.0000e+00 - 2s/epoch - 8ms/step\n",
      "Epoch 248/250\n",
      "291/291 - 2s - loss: 0.2048 - mean_squared_error: 0.2048 - accuracy: 0.0000e+00 - 2s/epoch - 7ms/step\n",
      "Epoch 249/250\n",
      "291/291 - 2s - loss: 0.2790 - mean_squared_error: 0.2790 - accuracy: 0.0000e+00 - 2s/epoch - 7ms/step\n",
      "Epoch 250/250\n",
      "291/291 - 3s - loss: 0.1939 - mean_squared_error: 0.1939 - accuracy: 0.0000e+00 - 3s/epoch - 9ms/step\n"
     ]
    }
   ],
   "source": [
    "\n",
    "history = model2.fit(X_train, # input data\n",
    "                    Y_train, # target data\n",
    "                    batch_size=2, # Number of samples per gradient update. If unspecified, batch_size will default to 32.\n",
    "                    epochs=250, # default=1, Number of epochs to train the model. An epoch is an iteration over the entire x and y data provided\n",
    "                    verbose=2, # default='auto', ('auto', 0, 1, or 2). Verbosity mode. 0 = silent, 1 = progress bar, 2 = one line per epoch. 'auto' defaults to 1 for most cases, but 2 when used with ParameterServerStrategy.\n",
    "                    callbacks=None, # default=None, list of callbacks to apply during training. See tf.keras.callbacks\n",
    "                    validation_split=0.2, # default=0.0, Fraction of the training data to be used as validation data. The model will set apart this fraction of the training data, will not train on it, and will evaluate the loss and any model metrics on this data at the end of each epoch. \n",
    "                    #validation_data=(X_test, y_test), # default=None, Data on which to evaluate the loss and any model metrics at the end of each epoch. \n",
    "                    shuffle=True, # default=True, Boolean (whether to shuffle the training data before each epoch) or str (for 'batch').\n",
    "                    class_weight=None, # default=None, Optional dictionary mapping class indices (integers) to a weight (float) value, used for weighting the loss function (during training only). This can be useful to tell the model to \"pay more attention\" to samples from an under-represented class.\n",
    "                    sample_weight=None, # default=None, Optional Numpy array of weights for the training samples, used for weighting the loss function (during training only).\n",
    "                    initial_epoch=0, # Integer, default=0, Epoch at which to start training (useful for resuming a previous training run).\n",
    "                    steps_per_epoch=None, # Integer or None, default=None, Total number of steps (batches of samples) before declaring one epoch finished and starting the next epoch. When training with input tensors such as TensorFlow data tensors, the default None is equal to the number of samples in your dataset divided by the batch size, or 1 if that cannot be determined. \n",
    "                    validation_steps=None, # Only relevant if validation_data is provided and is a tf.data dataset. Total number of steps (batches of samples) to draw before stopping when performing validation at the end of every epoch.\n",
    "                    validation_batch_size=None, # Integer or None, default=None, Number of samples per validation batch. If unspecified, will default to batch_size.\n",
    "                    validation_freq=100, # default=1, Only relevant if validation data is provided. If an integer, specifies how many training epochs to run before a new validation run is performed, e.g. validation_freq=2 runs validation every 2 epochs.\n",
    "                    max_queue_size=10, # default=10, Used for generator or keras.utils.Sequence input only. Maximum size for the generator queue. If unspecified, max_queue_size will default to 10.\n",
    "                    workers=1, # default=1, Used for generator or keras.utils.Sequence input only. Maximum number of processes to spin up when using process-based threading. If unspecified, workers will default to 1.\n",
    "                    use_multiprocessing=True, # default=False, Used for generator or keras.utils.Sequence input only. If True, use process-based threading. If unspecified, use_multiprocessing will default to False. \n",
    "                \n",
    "                   )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 4 - Plot loss over epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Gather the trained model's weight and bias.\n",
    "trained_weight = model2.get_weights()[0]\n",
    "trained_bias = model2.get_weights()[1]\n",
    "\n",
    "  # The list of epochs is stored separately from the \n",
    "  # rest of history.\n",
    "epochs = history.epoch\n",
    "  \n",
    "  # Gather the history (a snapshot) of each epoch.\n",
    "#hist = pd.DataFrame(history.history)\n",
    "\n",
    "  # Specifically gather the model's root mean \n",
    "  # squared error at each epoch. \n",
    "rmse = history.history[\"mean_squared_error\"]\n",
    "\n",
    "#plot_the_model(trained_weight, trained_bias, X_test, Y_train)\n",
    "plot_the_loss_curve(epochs, rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 5 - Print Performance Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------------------- Model Summary --------------------\n",
      "Model: \"biLSTM-Model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " Hidden-LSTM-Encoder-Layer (  (None, 128)              33792     \n",
      " Bidirectional)                                                  \n",
      "                                                                 \n",
      " Repeat-Vector-Layer (Repeat  (None, 10, 128)          0         \n",
      " Vector)                                                         \n",
      "                                                                 \n",
      " Hidden-LSTM-Decoder-Layer (  (None, 10, 128)          98816     \n",
      " Bidirectional)                                                  \n",
      "                                                                 \n",
      " Output-Layer (TimeDistribut  (None, 10, 1)            129       \n",
      " ed)                                                             \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 132,737\n",
      "Trainable params: 132,737\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "-------------------- Weights and Biases --------------------\n",
      "Too many parameters to print but you can use the code provided if needed\n",
      "\n",
      "-------------------- Evaluation on Training Data --------------------\n",
      "Final loss : 0.19391517341136932\n",
      "Final mean_squared_error : 0.19391508400440216\n",
      "Final accuracy : 0.0\n",
      "Final val_loss : 0.5005019903182983\n",
      "Final val_mean_squared_error : 0.5005019307136536\n",
      "Final val_accuracy : 0.0\n",
      "\n",
      "-------------------- Evaluation on Test Data --------------------\n",
      "6/6 [==============================] - 1s 7ms/step - loss: 0.3357 - mean_squared_error: 0.3357 - accuracy: 0.0000e+00\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\")\n",
    "print('-------------------- Model Summary --------------------')\n",
    "model2.summary() # print model summary\n",
    "print(\"\")\n",
    "print('-------------------- Weights and Biases --------------------')\n",
    "print(\"Too many parameters to print but you can use the code provided if needed\")\n",
    "print(\"\")\n",
    "#for layer in model.layers:\n",
    "#    print(layer.name)\n",
    "#    for item in layer.get_weights():\n",
    "#        print(\"  \", item)\n",
    "#print(\"\")\n",
    "\n",
    "# Print the last value in the evaluation metrics contained within history file\n",
    "print('-------------------- Evaluation on Training Data --------------------')\n",
    "for item in history.history:\n",
    "    print(\"Final\", item, \":\", history.history[item][-1])\n",
    "print(\"\")\n",
    "\n",
    "# Evaluate the model on the test data using \"evaluate\"\n",
    "print('-------------------- Evaluation on Test Data --------------------')\n",
    "results = model2.evaluate(X_test, Y_test)\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.utils import plot_model,model_to_dot\n",
    "plot_model(model2, show_shapes=True, to_file='plotmodelbilstm0810.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pydot.Dot at 0x1d52eae2950>"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_to_dot(model2,show_shapes=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.engine.sequential.Sequential at 0x1d52b5aca30>"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d3e10ef16274dd72e574b8fa73b58450b957d8421a2901baded3cca26fcf5dda"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
