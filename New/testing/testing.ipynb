{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow/Keras: 2.9.0\n",
      "pandas: 1.4.2\n",
      "numpy: 1.22.4\n",
      "sklearn: 1.1.1\n",
      "plotly: 5.9.0\n"
     ]
    }
   ],
   "source": [
    "# Tensorflow / Keras\n",
    "from tensorflow import keras # for building Neural Networks\n",
    "print('Tensorflow/Keras: %s' % keras.__version__) # print version\n",
    "from keras.models import Sequential # for creating a linear stack of layers for our Neural Network\n",
    "from keras import Input # for instantiating a keras tensor\n",
    "from keras.layers import Masking,Bidirectional, LSTM, RepeatVector, Dense, TimeDistributed,MaxPooling1D, Flatten, Conv1D,Conv2D,Dropout, MaxPooling2D # for creating layers inside the Neural Network\n",
    "from keras.optimizers import Adam , SGD\n",
    "# Data manipulation\n",
    "import pandas as pd # for data manipulation\n",
    "print('pandas: %s' % pd.__version__) # print version\n",
    "import numpy as np # for data manipulation\n",
    "print('numpy: %s' % np.__version__) # print version\n",
    "\n",
    "# Sklearn\n",
    "import sklearn\n",
    "print('sklearn: %s' % sklearn.__version__) # print version\n",
    "from sklearn.preprocessing import MinMaxScaler # for feature scaling\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Visualization\n",
    "import plotly \n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "print('plotly: %s' % plotly.__version__) # print version\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "#file accessing\n",
    "import os\n",
    "# time stuff\n",
    "from datetime import timedelta\n",
    "import calendar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defined the plot_the_model and plot_the_loss_curve functions.\n"
     ]
    }
   ],
   "source": [
    "#@title Define the plotting functions\n",
    "def plot_the_model(trained_weight, trained_bias, feature, label):\n",
    "  \"\"\"Plot the trained model against the training feature and label.\"\"\"\n",
    "\n",
    "  # Label the axes.\n",
    "  plt.xlabel(\"feature\")\n",
    "  plt.ylabel(\"label\")\n",
    "\n",
    "  # Plot the feature values vs. label values.\n",
    "  plt.scatter(feature, label)\n",
    "\n",
    "  # Create a red line representing the model. The red line starts\n",
    "  # at coordinates (x0, y0) and ends at coordinates (x1, y1).\n",
    "  x0 = 0\n",
    "  y0 = trained_bias\n",
    "  x1 = feature[-1]\n",
    "  y1 = trained_bias + (trained_weight * x1)\n",
    "  plt.plot([x0, x1], [y0, y1], c='r')\n",
    "\n",
    "  # Render the scatter plot and the red line.\n",
    "  plt.show()\n",
    "\n",
    "def plot_the_loss_curve(epochs, rmse):\n",
    "  \"\"\"Plot the loss curve, which shows loss vs. epoch.\"\"\"\n",
    "\n",
    "  plt.figure()\n",
    "  plt.xlabel(\"Epoch\")\n",
    "  plt.ylabel(\"Root Mean Squared Error\")\n",
    "  #epochs.remove(max(epochs))\n",
    "  \n",
    "  #rmse.drop(max(rmse))\n",
    "  plt.plot(epochs, rmse, label=\"Loss\")\n",
    "  plt.legend()\n",
    "  plt.ylim([min(rmse)*0.97, max(rmse)])\n",
    "  plt.show()\n",
    "\n",
    "print(\"Defined the plot_the_model and plot_the_loss_curve functions.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Read from file and establish dataframe and future functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('E:/DSFellowship/InSAR_data_south/displacement/export_dataframe1.csv')\n",
    "df=df.set_index([df.columns[0],df.columns[1]])\n",
    "df.columns=pd.to_datetime(df.columns, format='%Y%m%d')\n",
    "df=df.dropna(axis=0, how='all')#drop full nan rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Remove next line to include all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>2014-11-08</th>\n",
       "      <th>2014-12-02</th>\n",
       "      <th>2014-12-26</th>\n",
       "      <th>2015-02-12</th>\n",
       "      <th>2015-03-08</th>\n",
       "      <th>2015-04-01</th>\n",
       "      <th>2015-04-25</th>\n",
       "      <th>2015-05-19</th>\n",
       "      <th>2015-06-12</th>\n",
       "      <th>2015-07-06</th>\n",
       "      <th>...</th>\n",
       "      <th>2018-10-18</th>\n",
       "      <th>2018-10-30</th>\n",
       "      <th>2018-11-11</th>\n",
       "      <th>2018-11-17</th>\n",
       "      <th>2018-11-23</th>\n",
       "      <th>2018-12-05</th>\n",
       "      <th>2018-12-17</th>\n",
       "      <th>2018-12-29</th>\n",
       "      <th>2019-01-10</th>\n",
       "      <th>2019-01-22</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Longitude</th>\n",
       "      <th>Latitude</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">-120.5287</th>\n",
       "      <th>35.3850</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-10.7300</td>\n",
       "      <td>-1.23580</td>\n",
       "      <td>-0.71533</td>\n",
       "      <td>-2.5669</td>\n",
       "      <td>-2.88530</td>\n",
       "      <td>-4.8845</td>\n",
       "      <td>-1.44490</td>\n",
       "      <td>-2.67900</td>\n",
       "      <td>-0.074493</td>\n",
       "      <td>...</td>\n",
       "      <td>-8.4332</td>\n",
       "      <td>-6.1473</td>\n",
       "      <td>-6.7993</td>\n",
       "      <td>-7.1569</td>\n",
       "      <td>-9.6598</td>\n",
       "      <td>-11.040</td>\n",
       "      <td>-7.0263</td>\n",
       "      <td>-5.8773</td>\n",
       "      <td>-5.6599</td>\n",
       "      <td>-5.9405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35.3870</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-10.6410</td>\n",
       "      <td>-1.21250</td>\n",
       "      <td>-0.66436</td>\n",
       "      <td>-2.5908</td>\n",
       "      <td>-2.83890</td>\n",
       "      <td>-4.8826</td>\n",
       "      <td>-1.44440</td>\n",
       "      <td>-2.68830</td>\n",
       "      <td>-0.071874</td>\n",
       "      <td>...</td>\n",
       "      <td>-9.0533</td>\n",
       "      <td>-6.7852</td>\n",
       "      <td>-7.4212</td>\n",
       "      <td>-7.7779</td>\n",
       "      <td>-10.2810</td>\n",
       "      <td>-11.599</td>\n",
       "      <td>-7.5750</td>\n",
       "      <td>-6.4959</td>\n",
       "      <td>-6.2671</td>\n",
       "      <td>-6.5752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35.3890</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-10.6520</td>\n",
       "      <td>-1.22570</td>\n",
       "      <td>-0.59206</td>\n",
       "      <td>-2.5869</td>\n",
       "      <td>-2.93140</td>\n",
       "      <td>-4.9469</td>\n",
       "      <td>-1.54020</td>\n",
       "      <td>-2.73290</td>\n",
       "      <td>-0.053922</td>\n",
       "      <td>...</td>\n",
       "      <td>-8.3268</td>\n",
       "      <td>-6.0873</td>\n",
       "      <td>-6.7238</td>\n",
       "      <td>-7.0569</td>\n",
       "      <td>-9.5797</td>\n",
       "      <td>-10.909</td>\n",
       "      <td>-6.8630</td>\n",
       "      <td>-5.8036</td>\n",
       "      <td>-5.5724</td>\n",
       "      <td>-5.9050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35.3910</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-10.5710</td>\n",
       "      <td>-1.18590</td>\n",
       "      <td>-0.54365</td>\n",
       "      <td>-2.5515</td>\n",
       "      <td>-2.86520</td>\n",
       "      <td>-4.8772</td>\n",
       "      <td>-1.44970</td>\n",
       "      <td>-2.67960</td>\n",
       "      <td>0.032237</td>\n",
       "      <td>...</td>\n",
       "      <td>-8.6760</td>\n",
       "      <td>-6.4492</td>\n",
       "      <td>-7.0647</td>\n",
       "      <td>-7.3800</td>\n",
       "      <td>-9.9036</td>\n",
       "      <td>-11.229</td>\n",
       "      <td>-7.1620</td>\n",
       "      <td>-6.1635</td>\n",
       "      <td>-5.9354</td>\n",
       "      <td>-6.2427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35.3930</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-10.5400</td>\n",
       "      <td>-1.18640</td>\n",
       "      <td>-0.57563</td>\n",
       "      <td>-2.6055</td>\n",
       "      <td>-2.99100</td>\n",
       "      <td>-5.0036</td>\n",
       "      <td>-1.58420</td>\n",
       "      <td>-2.86350</td>\n",
       "      <td>-0.151480</td>\n",
       "      <td>...</td>\n",
       "      <td>-8.6378</td>\n",
       "      <td>-6.4279</td>\n",
       "      <td>-7.0139</td>\n",
       "      <td>-7.3224</td>\n",
       "      <td>-9.8620</td>\n",
       "      <td>-11.172</td>\n",
       "      <td>-7.0655</td>\n",
       "      <td>-6.1587</td>\n",
       "      <td>-5.8695</td>\n",
       "      <td>-6.2297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">-120.4967</th>\n",
       "      <th>36.9614</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-7.8305</td>\n",
       "      <td>-0.65177</td>\n",
       "      <td>1.86720</td>\n",
       "      <td>-2.1476</td>\n",
       "      <td>2.14400</td>\n",
       "      <td>-2.9981</td>\n",
       "      <td>-0.12881</td>\n",
       "      <td>1.27110</td>\n",
       "      <td>-5.373900</td>\n",
       "      <td>...</td>\n",
       "      <td>-26.3500</td>\n",
       "      <td>-23.0530</td>\n",
       "      <td>-24.5980</td>\n",
       "      <td>-25.2540</td>\n",
       "      <td>-28.7660</td>\n",
       "      <td>-27.981</td>\n",
       "      <td>-26.8790</td>\n",
       "      <td>-25.4060</td>\n",
       "      <td>-25.9260</td>\n",
       "      <td>-23.5910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36.9674</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-7.8122</td>\n",
       "      <td>-1.15920</td>\n",
       "      <td>1.40050</td>\n",
       "      <td>-2.8259</td>\n",
       "      <td>1.24900</td>\n",
       "      <td>-5.1431</td>\n",
       "      <td>-1.91850</td>\n",
       "      <td>-1.22810</td>\n",
       "      <td>-6.187200</td>\n",
       "      <td>...</td>\n",
       "      <td>-30.2290</td>\n",
       "      <td>-27.1960</td>\n",
       "      <td>-28.4500</td>\n",
       "      <td>-29.1750</td>\n",
       "      <td>-32.6080</td>\n",
       "      <td>-32.126</td>\n",
       "      <td>-31.0410</td>\n",
       "      <td>-29.7020</td>\n",
       "      <td>-30.2230</td>\n",
       "      <td>-27.9510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36.9734</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-7.9336</td>\n",
       "      <td>-1.05680</td>\n",
       "      <td>1.61270</td>\n",
       "      <td>-2.5663</td>\n",
       "      <td>1.38210</td>\n",
       "      <td>-4.2734</td>\n",
       "      <td>-1.21510</td>\n",
       "      <td>-0.75902</td>\n",
       "      <td>-4.539400</td>\n",
       "      <td>...</td>\n",
       "      <td>-30.8330</td>\n",
       "      <td>-27.7560</td>\n",
       "      <td>-29.0640</td>\n",
       "      <td>-29.8300</td>\n",
       "      <td>-33.2910</td>\n",
       "      <td>-32.985</td>\n",
       "      <td>-31.7080</td>\n",
       "      <td>-30.3620</td>\n",
       "      <td>-30.9390</td>\n",
       "      <td>-28.5540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36.9754</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-7.9639</td>\n",
       "      <td>-1.12450</td>\n",
       "      <td>1.52230</td>\n",
       "      <td>-2.7758</td>\n",
       "      <td>1.24800</td>\n",
       "      <td>-4.7358</td>\n",
       "      <td>-1.92040</td>\n",
       "      <td>-1.34800</td>\n",
       "      <td>-5.269600</td>\n",
       "      <td>...</td>\n",
       "      <td>-31.5390</td>\n",
       "      <td>-28.4460</td>\n",
       "      <td>-29.8040</td>\n",
       "      <td>-30.5780</td>\n",
       "      <td>-34.0430</td>\n",
       "      <td>-33.740</td>\n",
       "      <td>-32.3790</td>\n",
       "      <td>-31.0350</td>\n",
       "      <td>-31.6230</td>\n",
       "      <td>-29.2010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36.9774</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-7.8965</td>\n",
       "      <td>-1.08890</td>\n",
       "      <td>1.77080</td>\n",
       "      <td>-2.7975</td>\n",
       "      <td>0.88628</td>\n",
       "      <td>-4.1611</td>\n",
       "      <td>-1.11250</td>\n",
       "      <td>-0.49866</td>\n",
       "      <td>-4.281300</td>\n",
       "      <td>...</td>\n",
       "      <td>-29.0880</td>\n",
       "      <td>-25.9970</td>\n",
       "      <td>-27.2310</td>\n",
       "      <td>-27.9980</td>\n",
       "      <td>-31.3850</td>\n",
       "      <td>-31.267</td>\n",
       "      <td>-29.8260</td>\n",
       "      <td>-28.5620</td>\n",
       "      <td>-29.1080</td>\n",
       "      <td>-26.7680</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20000 rows Ã— 110 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    2014-11-08  2014-12-02  2014-12-26  2015-02-12  \\\n",
       "Longitude Latitude                                                   \n",
       "-120.5287 35.3850          0.0    -10.7300    -1.23580    -0.71533   \n",
       "          35.3870          0.0    -10.6410    -1.21250    -0.66436   \n",
       "          35.3890          0.0    -10.6520    -1.22570    -0.59206   \n",
       "          35.3910          0.0    -10.5710    -1.18590    -0.54365   \n",
       "          35.3930          0.0    -10.5400    -1.18640    -0.57563   \n",
       "...                        ...         ...         ...         ...   \n",
       "-120.4967 36.9614          0.0     -7.8305    -0.65177     1.86720   \n",
       "          36.9674          0.0     -7.8122    -1.15920     1.40050   \n",
       "          36.9734          0.0     -7.9336    -1.05680     1.61270   \n",
       "          36.9754          0.0     -7.9639    -1.12450     1.52230   \n",
       "          36.9774          0.0     -7.8965    -1.08890     1.77080   \n",
       "\n",
       "                    2015-03-08  2015-04-01  2015-04-25  2015-05-19  \\\n",
       "Longitude Latitude                                                   \n",
       "-120.5287 35.3850      -2.5669    -2.88530     -4.8845    -1.44490   \n",
       "          35.3870      -2.5908    -2.83890     -4.8826    -1.44440   \n",
       "          35.3890      -2.5869    -2.93140     -4.9469    -1.54020   \n",
       "          35.3910      -2.5515    -2.86520     -4.8772    -1.44970   \n",
       "          35.3930      -2.6055    -2.99100     -5.0036    -1.58420   \n",
       "...                        ...         ...         ...         ...   \n",
       "-120.4967 36.9614      -2.1476     2.14400     -2.9981    -0.12881   \n",
       "          36.9674      -2.8259     1.24900     -5.1431    -1.91850   \n",
       "          36.9734      -2.5663     1.38210     -4.2734    -1.21510   \n",
       "          36.9754      -2.7758     1.24800     -4.7358    -1.92040   \n",
       "          36.9774      -2.7975     0.88628     -4.1611    -1.11250   \n",
       "\n",
       "                    2015-06-12  2015-07-06  ...  2018-10-18  2018-10-30  \\\n",
       "Longitude Latitude                          ...                           \n",
       "-120.5287 35.3850     -2.67900   -0.074493  ...     -8.4332     -6.1473   \n",
       "          35.3870     -2.68830   -0.071874  ...     -9.0533     -6.7852   \n",
       "          35.3890     -2.73290   -0.053922  ...     -8.3268     -6.0873   \n",
       "          35.3910     -2.67960    0.032237  ...     -8.6760     -6.4492   \n",
       "          35.3930     -2.86350   -0.151480  ...     -8.6378     -6.4279   \n",
       "...                        ...         ...  ...         ...         ...   \n",
       "-120.4967 36.9614      1.27110   -5.373900  ...    -26.3500    -23.0530   \n",
       "          36.9674     -1.22810   -6.187200  ...    -30.2290    -27.1960   \n",
       "          36.9734     -0.75902   -4.539400  ...    -30.8330    -27.7560   \n",
       "          36.9754     -1.34800   -5.269600  ...    -31.5390    -28.4460   \n",
       "          36.9774     -0.49866   -4.281300  ...    -29.0880    -25.9970   \n",
       "\n",
       "                    2018-11-11  2018-11-17  2018-11-23  2018-12-05  \\\n",
       "Longitude Latitude                                                   \n",
       "-120.5287 35.3850      -6.7993     -7.1569     -9.6598     -11.040   \n",
       "          35.3870      -7.4212     -7.7779    -10.2810     -11.599   \n",
       "          35.3890      -6.7238     -7.0569     -9.5797     -10.909   \n",
       "          35.3910      -7.0647     -7.3800     -9.9036     -11.229   \n",
       "          35.3930      -7.0139     -7.3224     -9.8620     -11.172   \n",
       "...                        ...         ...         ...         ...   \n",
       "-120.4967 36.9614     -24.5980    -25.2540    -28.7660     -27.981   \n",
       "          36.9674     -28.4500    -29.1750    -32.6080     -32.126   \n",
       "          36.9734     -29.0640    -29.8300    -33.2910     -32.985   \n",
       "          36.9754     -29.8040    -30.5780    -34.0430     -33.740   \n",
       "          36.9774     -27.2310    -27.9980    -31.3850     -31.267   \n",
       "\n",
       "                    2018-12-17  2018-12-29  2019-01-10  2019-01-22  \n",
       "Longitude Latitude                                                  \n",
       "-120.5287 35.3850      -7.0263     -5.8773     -5.6599     -5.9405  \n",
       "          35.3870      -7.5750     -6.4959     -6.2671     -6.5752  \n",
       "          35.3890      -6.8630     -5.8036     -5.5724     -5.9050  \n",
       "          35.3910      -7.1620     -6.1635     -5.9354     -6.2427  \n",
       "          35.3930      -7.0655     -6.1587     -5.8695     -6.2297  \n",
       "...                        ...         ...         ...         ...  \n",
       "-120.4967 36.9614     -26.8790    -25.4060    -25.9260    -23.5910  \n",
       "          36.9674     -31.0410    -29.7020    -30.2230    -27.9510  \n",
       "          36.9734     -31.7080    -30.3620    -30.9390    -28.5540  \n",
       "          36.9754     -32.3790    -31.0350    -31.6230    -29.2010  \n",
       "          36.9774     -29.8260    -28.5620    -29.1080    -26.7680  \n",
       "\n",
       "[20000 rows x 110 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=df.iloc[120000:140000] #cuts data to long 120-119 approx\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Timestep and features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestep=10\n",
    "features=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shaping function (remove start and step for first for loop to include all data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shaping(datain, timestep):\n",
    "    #print(arr)\n",
    "    cnt=0\n",
    "    \n",
    "    for row in range(0,len(datain.index),1750): #picks a row at every iteration, allows to reduction of input and inclusion of multiple time series, remove start and step to run on full dataset \n",
    "    # Convert input dataframe to array and flatten\n",
    "        arr=datain.iloc[row].to_numpy().flatten() # flatten row\n",
    "        arr[np.isnan(arr)] = -1000\n",
    "        for mth in range(0, len(datain.columns)-(2*timestep)+1): # Define range lenght of the dates - 2* amount of timesep?? +1\n",
    "            cnt=cnt+1 # Gives us the number of samples. Later used to reshape the data\n",
    "            X_start=mth # Start month for inputs of each sample\n",
    "            X_end=mth+timestep # End month for inputs of each sample\n",
    "            Y_start=mth+timestep # Start month for targets of each sample. Note, start is inclusive and end is exclusive, that's why X_end and Y_start is the same number\n",
    "            Y_end=mth+2*timestep # End month for targets of each sample.  \n",
    "            \n",
    "            # Assemble input and target arrays containing all samples\n",
    "            if cnt==1:\n",
    "                X_comb=arr[X_start:X_end]\n",
    "                Y_comb=arr[Y_start:Y_end]\n",
    "            else: \n",
    "                X_comb=np.append(X_comb, arr[X_start:X_end])\n",
    "                Y_comb=np.append(Y_comb, arr[Y_start:Y_end])\n",
    "    \n",
    "    # Reshape input and target arrays\n",
    "    X_out=np.reshape(X_comb, (cnt, timestep, 1))\n",
    "    Y_out=np.reshape(Y_comb, (cnt, timestep, 1))\n",
    "    return X_out, Y_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 2 - Split and Shape data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Divides data into 80% training 20% testing\n",
    "train, test = train_test_split(df, test_size=0.2)\n",
    "# creates sequences to train\n",
    "X_train, Y_train = shaping(datain=train, timestep= timestep )\n",
    "X_test, Y_test = shaping(datain=test, timestep=timestep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 3 - Specify the structure of a Neural Network, first model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = Sequential(name=\"LSTM-Model\") # Model\n",
    "#model2.add(Masking(mask_value=-1000, input_shape=(timestep, 1)))\n",
    "model2.add(Input(shape=(X_train.shape[1],X_train.shape[2]), name='Input-Layer')) # Input Layer - need to speicfy the shape of inputs\n",
    "model2.add(Bidirectional(LSTM(units=64, activation='relu', recurrent_activation='sigmoid', stateful=False), name='Hidden-LSTM-Encoder-Layer')) # Encoder Layer\n",
    "model2.add(RepeatVector(Y_train.shape[1], name='Repeat-Vector-Layer')) # Repeat Vector\n",
    "model2.add(Bidirectional(LSTM(units=64, activation='relu', recurrent_activation='sigmoid', stateful=False, return_sequences=True), name='Hidden-LSTM-Decoder-Layer')) # Decoder Layer\n",
    "model2.add(TimeDistributed(Dense(units=1, activation='linear'), name='Output-Layer')) # Output Layer, Linear(x) = x\n",
    "#optimizer=Adam(.001)\n",
    "model2.compile(optimizer='adam', # default='rmsprop', an algorithm to be used in backpropagation\n",
    "              loss='mean_squared_error', # Loss function to be optimized. A string (name of loss function), or a tf.keras.losses.Loss instance.\n",
    "              metrics=['MeanSquaredError', 'MeanAbsoluteError'], # List of metrics to be evaluated by the model during training and testing. Each of this can be a string (name of a built-in function), function or a tf.keras.metrics.Metric instance. \n",
    "              loss_weights=None, # default=None, Optional list or dictionary specifying scalar coefficients (Python floats) to weight the loss contributions of different model outputs.\n",
    "              weighted_metrics=None, # default=None, List of metrics to be evaluated and weighted by sample_weight or class_weight during training and testing.\n",
    "              run_eagerly=None, # Defaults to False. If True, this Model's logic will not be wrapped in a tf.function. Recommended to leave this as None unless your Model cannot be run inside a tf.function.\n",
    "              steps_per_execution=None # Defaults to 1. The number of batches to run during each tf.function call. Running multiple batches inside a single tf.function call can greatly improve performance on TPUs or small models with a large Python overhead.\n",
    "        \n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/140\n",
      "243/243 - 35s - loss: 9477.8799 - mean_squared_error: 9477.8857 - mean_absolute_error: 15.5016 - 35s/epoch - 143ms/step\n",
      "Epoch 2/140\n",
      "243/243 - 11s - loss: 9413.4463 - mean_squared_error: 9413.4473 - mean_absolute_error: 17.5332 - 11s/epoch - 47ms/step\n",
      "Epoch 3/140\n",
      "243/243 - 11s - loss: 9381.3857 - mean_squared_error: 9381.3838 - mean_absolute_error: 18.2983 - 11s/epoch - 44ms/step\n",
      "Epoch 4/140\n",
      "243/243 - 12s - loss: 9310.2324 - mean_squared_error: 9310.2354 - mean_absolute_error: 18.2610 - 12s/epoch - 47ms/step\n",
      "Epoch 5/140\n",
      "243/243 - 11s - loss: 9292.6738 - mean_squared_error: 9292.6768 - mean_absolute_error: 18.5272 - 11s/epoch - 44ms/step\n",
      "Epoch 6/140\n",
      "243/243 - 11s - loss: 9316.7646 - mean_squared_error: 9316.7627 - mean_absolute_error: 18.7201 - 11s/epoch - 46ms/step\n",
      "Epoch 7/140\n",
      "243/243 - 11s - loss: 9268.2178 - mean_squared_error: 9268.2207 - mean_absolute_error: 17.9480 - 11s/epoch - 47ms/step\n",
      "Epoch 8/140\n",
      "243/243 - 6s - loss: 9164.9082 - mean_squared_error: 9164.9092 - mean_absolute_error: 18.2415 - 6s/epoch - 25ms/step\n",
      "Epoch 9/140\n",
      "243/243 - 5s - loss: 9165.5859 - mean_squared_error: 9165.5840 - mean_absolute_error: 18.9860 - 5s/epoch - 19ms/step\n",
      "Epoch 10/140\n",
      "243/243 - 5s - loss: 9054.0518 - mean_squared_error: 9054.0537 - mean_absolute_error: 18.6879 - 5s/epoch - 19ms/step\n",
      "Epoch 11/140\n",
      "243/243 - 5s - loss: 15310.3389 - mean_squared_error: 15310.3428 - mean_absolute_error: 18.9988 - 5s/epoch - 19ms/step\n",
      "Epoch 12/140\n",
      "243/243 - 4s - loss: 9485.6709 - mean_squared_error: 9485.6768 - mean_absolute_error: 16.7476 - 4s/epoch - 18ms/step\n",
      "Epoch 13/140\n",
      "243/243 - 4s - loss: 9253.0029 - mean_squared_error: 9253.0049 - mean_absolute_error: 17.8348 - 4s/epoch - 18ms/step\n",
      "Epoch 14/140\n",
      "243/243 - 5s - loss: 9387.8291 - mean_squared_error: 9387.8281 - mean_absolute_error: 18.5877 - 5s/epoch - 19ms/step\n",
      "Epoch 15/140\n",
      "243/243 - 4s - loss: 9940.1211 - mean_squared_error: 9940.1191 - mean_absolute_error: 17.3007 - 4s/epoch - 18ms/step\n",
      "Epoch 16/140\n",
      "243/243 - 5s - loss: 9146.5225 - mean_squared_error: 9146.5195 - mean_absolute_error: 18.3260 - 5s/epoch - 19ms/step\n",
      "Epoch 17/140\n",
      "243/243 - 4s - loss: 9121.7354 - mean_squared_error: 9121.7344 - mean_absolute_error: 18.9438 - 4s/epoch - 18ms/step\n",
      "Epoch 18/140\n",
      "243/243 - 5s - loss: 9044.2158 - mean_squared_error: 9044.2158 - mean_absolute_error: 18.1522 - 5s/epoch - 19ms/step\n",
      "Epoch 19/140\n",
      "243/243 - 4s - loss: 8959.8252 - mean_squared_error: 8959.8262 - mean_absolute_error: 18.6490 - 4s/epoch - 17ms/step\n",
      "Epoch 20/140\n",
      "243/243 - 4s - loss: 8966.7539 - mean_squared_error: 8966.7520 - mean_absolute_error: 18.8858 - 4s/epoch - 17ms/step\n",
      "Epoch 21/140\n",
      "243/243 - 4s - loss: 8891.4922 - mean_squared_error: 8891.4922 - mean_absolute_error: 18.5888 - 4s/epoch - 17ms/step\n",
      "Epoch 22/140\n",
      "243/243 - 4s - loss: 8888.3936 - mean_squared_error: 8888.3887 - mean_absolute_error: 18.0750 - 4s/epoch - 17ms/step\n",
      "Epoch 23/140\n",
      "243/243 - 4s - loss: 8851.2178 - mean_squared_error: 8851.2178 - mean_absolute_error: 18.6348 - 4s/epoch - 18ms/step\n",
      "Epoch 24/140\n",
      "243/243 - 4s - loss: 8771.1172 - mean_squared_error: 8771.1240 - mean_absolute_error: 18.2586 - 4s/epoch - 18ms/step\n",
      "Epoch 25/140\n",
      "243/243 - 4s - loss: 9084.0957 - mean_squared_error: 9084.0977 - mean_absolute_error: 16.9229 - 4s/epoch - 17ms/step\n",
      "Epoch 26/140\n",
      "243/243 - 4s - loss: 8759.0684 - mean_squared_error: 8759.0674 - mean_absolute_error: 19.1935 - 4s/epoch - 17ms/step\n",
      "Epoch 27/140\n",
      "243/243 - 4s - loss: 8537.8672 - mean_squared_error: 8537.8633 - mean_absolute_error: 18.9125 - 4s/epoch - 17ms/step\n",
      "Epoch 28/140\n",
      "243/243 - 4s - loss: 9164.4922 - mean_squared_error: 9164.4922 - mean_absolute_error: 17.7994 - 4s/epoch - 17ms/step\n",
      "Epoch 29/140\n",
      "243/243 - 4s - loss: 9389.8760 - mean_squared_error: 9389.8721 - mean_absolute_error: 16.7820 - 4s/epoch - 17ms/step\n",
      "Epoch 30/140\n",
      "243/243 - 4s - loss: 9319.8223 - mean_squared_error: 9319.8252 - mean_absolute_error: 18.0051 - 4s/epoch - 18ms/step\n",
      "Epoch 31/140\n",
      "243/243 - 4s - loss: 9230.8398 - mean_squared_error: 9230.8428 - mean_absolute_error: 18.6082 - 4s/epoch - 17ms/step\n",
      "Epoch 32/140\n",
      "243/243 - 4s - loss: 9127.2451 - mean_squared_error: 9127.2432 - mean_absolute_error: 19.0282 - 4s/epoch - 18ms/step\n",
      "Epoch 33/140\n",
      "243/243 - 5s - loss: 9111.7607 - mean_squared_error: 9111.7568 - mean_absolute_error: 20.5278 - 5s/epoch - 19ms/step\n",
      "Epoch 34/140\n",
      "243/243 - 4s - loss: 8796.8232 - mean_squared_error: 8796.8242 - mean_absolute_error: 20.5096 - 4s/epoch - 18ms/step\n",
      "Epoch 35/140\n",
      "243/243 - 5s - loss: 9189.6533 - mean_squared_error: 9189.6543 - mean_absolute_error: 19.0591 - 5s/epoch - 19ms/step\n",
      "Epoch 36/140\n",
      "243/243 - 4s - loss: 8795.1934 - mean_squared_error: 8795.1973 - mean_absolute_error: 19.8265 - 4s/epoch - 18ms/step\n",
      "Epoch 37/140\n",
      "243/243 - 4s - loss: 8953.2373 - mean_squared_error: 8953.2393 - mean_absolute_error: 19.2988 - 4s/epoch - 18ms/step\n",
      "Epoch 38/140\n",
      "243/243 - 4s - loss: 8907.1973 - mean_squared_error: 8907.1982 - mean_absolute_error: 21.1215 - 4s/epoch - 18ms/step\n",
      "Epoch 39/140\n",
      "243/243 - 5s - loss: 8588.1445 - mean_squared_error: 8588.1436 - mean_absolute_error: 19.1789 - 5s/epoch - 19ms/step\n",
      "Epoch 40/140\n",
      "243/243 - 5s - loss: 8349.4893 - mean_squared_error: 8349.4883 - mean_absolute_error: 20.1828 - 5s/epoch - 21ms/step\n",
      "Epoch 41/140\n",
      "243/243 - 5s - loss: 8742.0664 - mean_squared_error: 8742.0674 - mean_absolute_error: 20.7501 - 5s/epoch - 21ms/step\n",
      "Epoch 42/140\n",
      "243/243 - 5s - loss: 8558.2979 - mean_squared_error: 8558.2988 - mean_absolute_error: 19.9378 - 5s/epoch - 20ms/step\n",
      "Epoch 43/140\n",
      "243/243 - 5s - loss: 8821.8838 - mean_squared_error: 8821.8848 - mean_absolute_error: 18.7023 - 5s/epoch - 20ms/step\n",
      "Epoch 44/140\n",
      "243/243 - 4s - loss: 8660.5361 - mean_squared_error: 8660.5342 - mean_absolute_error: 18.6563 - 4s/epoch - 18ms/step\n",
      "Epoch 45/140\n",
      "243/243 - 4s - loss: 8831.8877 - mean_squared_error: 8831.8838 - mean_absolute_error: 20.0789 - 4s/epoch - 17ms/step\n",
      "Epoch 46/140\n",
      "243/243 - 4s - loss: 9361.9189 - mean_squared_error: 9361.9219 - mean_absolute_error: 19.7072 - 4s/epoch - 17ms/step\n",
      "Epoch 47/140\n",
      "243/243 - 4s - loss: 9268.7207 - mean_squared_error: 9268.7188 - mean_absolute_error: 19.9995 - 4s/epoch - 17ms/step\n",
      "Epoch 48/140\n",
      "243/243 - 4s - loss: 9148.1230 - mean_squared_error: 9148.1250 - mean_absolute_error: 18.9004 - 4s/epoch - 17ms/step\n",
      "Epoch 49/140\n",
      "243/243 - 4s - loss: 8923.7539 - mean_squared_error: 8923.7529 - mean_absolute_error: 18.4976 - 4s/epoch - 17ms/step\n",
      "Epoch 50/140\n",
      "243/243 - 4s - loss: 9224.2139 - mean_squared_error: 9224.2119 - mean_absolute_error: 19.9503 - 4s/epoch - 17ms/step\n",
      "Epoch 51/140\n",
      "243/243 - 4s - loss: 8841.4414 - mean_squared_error: 8841.4385 - mean_absolute_error: 19.2776 - 4s/epoch - 17ms/step\n",
      "Epoch 52/140\n",
      "243/243 - 4s - loss: 8621.0898 - mean_squared_error: 8621.0947 - mean_absolute_error: 19.5332 - 4s/epoch - 17ms/step\n",
      "Epoch 53/140\n",
      "243/243 - 4s - loss: 8445.2373 - mean_squared_error: 8445.2354 - mean_absolute_error: 19.6812 - 4s/epoch - 16ms/step\n",
      "Epoch 54/140\n",
      "243/243 - 4s - loss: 8338.6514 - mean_squared_error: 8338.6514 - mean_absolute_error: 18.6354 - 4s/epoch - 17ms/step\n",
      "Epoch 55/140\n",
      "243/243 - 4s - loss: 8083.3242 - mean_squared_error: 8083.3184 - mean_absolute_error: 19.4018 - 4s/epoch - 17ms/step\n",
      "Epoch 56/140\n",
      "243/243 - 4s - loss: 8881.7910 - mean_squared_error: 8881.7920 - mean_absolute_error: 19.1523 - 4s/epoch - 17ms/step\n",
      "Epoch 57/140\n",
      "243/243 - 7s - loss: 9430.5293 - mean_squared_error: 9430.5312 - mean_absolute_error: 19.2398 - 7s/epoch - 27ms/step\n",
      "Epoch 58/140\n",
      "243/243 - 10s - loss: 9255.7646 - mean_squared_error: 9255.7607 - mean_absolute_error: 19.6234 - 10s/epoch - 42ms/step\n",
      "Epoch 59/140\n",
      "243/243 - 9s - loss: 9178.3301 - mean_squared_error: 9178.3320 - mean_absolute_error: 20.2266 - 9s/epoch - 38ms/step\n",
      "Epoch 60/140\n",
      "243/243 - 10s - loss: 9140.9639 - mean_squared_error: 9140.9629 - mean_absolute_error: 21.6688 - 10s/epoch - 43ms/step\n",
      "Epoch 61/140\n",
      "243/243 - 10s - loss: 8928.2490 - mean_squared_error: 8928.2539 - mean_absolute_error: 20.4648 - 10s/epoch - 43ms/step\n",
      "Epoch 62/140\n",
      "243/243 - 12s - loss: 8458.6641 - mean_squared_error: 8458.6641 - mean_absolute_error: 19.4406 - 12s/epoch - 48ms/step\n",
      "Epoch 63/140\n",
      "243/243 - 11s - loss: 8699.9990 - mean_squared_error: 8700.0020 - mean_absolute_error: 18.7562 - 11s/epoch - 47ms/step\n",
      "Epoch 64/140\n",
      "243/243 - 11s - loss: 9418.2969 - mean_squared_error: 9418.3008 - mean_absolute_error: 18.2632 - 11s/epoch - 46ms/step\n",
      "Epoch 65/140\n",
      "243/243 - 12s - loss: 9296.3877 - mean_squared_error: 9296.3936 - mean_absolute_error: 19.4284 - 12s/epoch - 49ms/step\n",
      "Epoch 66/140\n",
      "243/243 - 11s - loss: 9194.5547 - mean_squared_error: 9194.5557 - mean_absolute_error: 19.4826 - 11s/epoch - 45ms/step\n",
      "Epoch 67/140\n",
      "243/243 - 12s - loss: 9421.6680 - mean_squared_error: 9421.6670 - mean_absolute_error: 19.8481 - 12s/epoch - 50ms/step\n",
      "Epoch 68/140\n",
      "243/243 - 10s - loss: 9325.7812 - mean_squared_error: 9325.7803 - mean_absolute_error: 19.1047 - 10s/epoch - 41ms/step\n",
      "Epoch 69/140\n",
      "243/243 - 4s - loss: 9141.0439 - mean_squared_error: 9141.0479 - mean_absolute_error: 20.5386 - 4s/epoch - 18ms/step\n",
      "Epoch 70/140\n",
      "243/243 - 4s - loss: 9440.7422 - mean_squared_error: 9440.7480 - mean_absolute_error: 20.5501 - 4s/epoch - 17ms/step\n",
      "Epoch 71/140\n",
      "243/243 - 4s - loss: 9259.4238 - mean_squared_error: 9259.4229 - mean_absolute_error: 20.0709 - 4s/epoch - 17ms/step\n",
      "Epoch 72/140\n",
      "243/243 - 4s - loss: 9163.2354 - mean_squared_error: 9163.2383 - mean_absolute_error: 19.6833 - 4s/epoch - 17ms/step\n",
      "Epoch 73/140\n",
      "243/243 - 4s - loss: 9158.5566 - mean_squared_error: 9158.5625 - mean_absolute_error: 21.2577 - 4s/epoch - 18ms/step\n",
      "Epoch 74/140\n",
      "243/243 - 4s - loss: 9136.6582 - mean_squared_error: 9136.6582 - mean_absolute_error: 19.4230 - 4s/epoch - 17ms/step\n",
      "Epoch 75/140\n",
      "243/243 - 4s - loss: 8960.5986 - mean_squared_error: 8960.5957 - mean_absolute_error: 19.4384 - 4s/epoch - 17ms/step\n",
      "Epoch 76/140\n",
      "243/243 - 4s - loss: 9100.0947 - mean_squared_error: 9100.0996 - mean_absolute_error: 19.5450 - 4s/epoch - 17ms/step\n",
      "Epoch 77/140\n",
      "243/243 - 4s - loss: 8916.4141 - mean_squared_error: 8916.4141 - mean_absolute_error: 19.9815 - 4s/epoch - 18ms/step\n",
      "Epoch 78/140\n",
      "243/243 - 4s - loss: 9109.1494 - mean_squared_error: 9109.1504 - mean_absolute_error: 19.5205 - 4s/epoch - 18ms/step\n",
      "Epoch 79/140\n",
      "243/243 - 5s - loss: 9117.9688 - mean_squared_error: 9117.9736 - mean_absolute_error: 21.8462 - 5s/epoch - 20ms/step\n",
      "Epoch 80/140\n",
      "243/243 - 5s - loss: 9278.6260 - mean_squared_error: 9278.6318 - mean_absolute_error: 21.6355 - 5s/epoch - 20ms/step\n",
      "Epoch 81/140\n",
      "243/243 - 5s - loss: 9205.1367 - mean_squared_error: 9205.1377 - mean_absolute_error: 19.4875 - 5s/epoch - 20ms/step\n",
      "Epoch 82/140\n",
      "243/243 - 4s - loss: 9057.3447 - mean_squared_error: 9057.3457 - mean_absolute_error: 19.9198 - 4s/epoch - 18ms/step\n",
      "Epoch 83/140\n",
      "243/243 - 4s - loss: 8953.9668 - mean_squared_error: 8953.9678 - mean_absolute_error: 20.6053 - 4s/epoch - 18ms/step\n",
      "Epoch 84/140\n",
      "243/243 - 4s - loss: 8937.0566 - mean_squared_error: 8937.0615 - mean_absolute_error: 20.8469 - 4s/epoch - 17ms/step\n",
      "Epoch 85/140\n",
      "243/243 - 4s - loss: 8537.8857 - mean_squared_error: 8537.8828 - mean_absolute_error: 19.7358 - 4s/epoch - 17ms/step\n",
      "Epoch 86/140\n",
      "243/243 - 4s - loss: 8281.5117 - mean_squared_error: 8281.5137 - mean_absolute_error: 18.7254 - 4s/epoch - 18ms/step\n",
      "Epoch 87/140\n",
      "243/243 - 4s - loss: 8239.6748 - mean_squared_error: 8239.6729 - mean_absolute_error: 19.0797 - 4s/epoch - 18ms/step\n",
      "Epoch 88/140\n",
      "243/243 - 4s - loss: 8351.3477 - mean_squared_error: 8351.3486 - mean_absolute_error: 19.4939 - 4s/epoch - 17ms/step\n",
      "Epoch 89/140\n",
      "243/243 - 4s - loss: 8491.4902 - mean_squared_error: 8491.4961 - mean_absolute_error: 18.6382 - 4s/epoch - 18ms/step\n",
      "Epoch 90/140\n",
      "243/243 - 4s - loss: 7928.0234 - mean_squared_error: 7928.0269 - mean_absolute_error: 17.7055 - 4s/epoch - 17ms/step\n",
      "Epoch 91/140\n",
      "243/243 - 4s - loss: 8017.9204 - mean_squared_error: 8017.9224 - mean_absolute_error: 18.2152 - 4s/epoch - 17ms/step\n",
      "Epoch 92/140\n",
      "243/243 - 4s - loss: 8573.6768 - mean_squared_error: 8573.6768 - mean_absolute_error: 19.3551 - 4s/epoch - 17ms/step\n",
      "Epoch 93/140\n",
      "243/243 - 4s - loss: 9634.1885 - mean_squared_error: 9634.1943 - mean_absolute_error: 19.9591 - 4s/epoch - 17ms/step\n",
      "Epoch 94/140\n",
      "243/243 - 4s - loss: 9356.5947 - mean_squared_error: 9356.5947 - mean_absolute_error: 17.1762 - 4s/epoch - 17ms/step\n",
      "Epoch 95/140\n",
      "243/243 - 4s - loss: 9374.8691 - mean_squared_error: 9374.8672 - mean_absolute_error: 16.9943 - 4s/epoch - 17ms/step\n",
      "Epoch 96/140\n",
      "243/243 - 4s - loss: 9300.8047 - mean_squared_error: 9300.8047 - mean_absolute_error: 17.8028 - 4s/epoch - 17ms/step\n",
      "Epoch 97/140\n",
      "243/243 - 4s - loss: 9204.0635 - mean_squared_error: 9204.0674 - mean_absolute_error: 19.6984 - 4s/epoch - 17ms/step\n",
      "Epoch 98/140\n",
      "243/243 - 4s - loss: 9855.8633 - mean_squared_error: 9855.8672 - mean_absolute_error: 17.9433 - 4s/epoch - 17ms/step\n",
      "Epoch 99/140\n",
      "243/243 - 4s - loss: 9390.3457 - mean_squared_error: 9390.3438 - mean_absolute_error: 16.5397 - 4s/epoch - 17ms/step\n",
      "Epoch 100/140\n",
      "243/243 - 6s - loss: 9853.0586 - mean_squared_error: 9853.0557 - mean_absolute_error: 21.1921 - val_loss: 10865.3779 - val_mean_squared_error: 10865.3779 - val_mean_absolute_error: 18.6990 - 6s/epoch - 26ms/step\n",
      "Epoch 101/140\n",
      "243/243 - 4s - loss: 9436.8232 - mean_squared_error: 9436.8252 - mean_absolute_error: 20.8438 - 4s/epoch - 16ms/step\n",
      "Epoch 102/140\n",
      "243/243 - 4s - loss: 9377.3369 - mean_squared_error: 9377.3369 - mean_absolute_error: 20.1281 - 4s/epoch - 17ms/step\n",
      "Epoch 103/140\n",
      "243/243 - 4s - loss: 13855.6045 - mean_squared_error: 13855.6045 - mean_absolute_error: 26.7260 - 4s/epoch - 17ms/step\n",
      "Epoch 104/140\n",
      "243/243 - 4s - loss: 9342.8232 - mean_squared_error: 9342.8145 - mean_absolute_error: 19.9109 - 4s/epoch - 17ms/step\n",
      "Epoch 105/140\n",
      "243/243 - 4s - loss: 9280.7959 - mean_squared_error: 9280.7959 - mean_absolute_error: 18.8350 - 4s/epoch - 17ms/step\n",
      "Epoch 106/140\n",
      "243/243 - 4s - loss: 9271.3535 - mean_squared_error: 9271.3564 - mean_absolute_error: 18.0589 - 4s/epoch - 17ms/step\n",
      "Epoch 107/140\n",
      "243/243 - 4s - loss: 9303.7539 - mean_squared_error: 9303.7568 - mean_absolute_error: 18.1518 - 4s/epoch - 17ms/step\n",
      "Epoch 108/140\n",
      "243/243 - 4s - loss: 9269.2979 - mean_squared_error: 9269.2969 - mean_absolute_error: 18.5917 - 4s/epoch - 17ms/step\n",
      "Epoch 109/140\n",
      "243/243 - 4s - loss: 9242.9961 - mean_squared_error: 9242.9922 - mean_absolute_error: 19.9882 - 4s/epoch - 17ms/step\n",
      "Epoch 110/140\n",
      "243/243 - 4s - loss: 9233.8203 - mean_squared_error: 9233.8184 - mean_absolute_error: 19.7357 - 4s/epoch - 17ms/step\n",
      "Epoch 111/140\n",
      "243/243 - 4s - loss: 9248.2188 - mean_squared_error: 9248.2178 - mean_absolute_error: 21.9913 - 4s/epoch - 17ms/step\n",
      "Epoch 112/140\n",
      "243/243 - 5s - loss: 9324.2412 - mean_squared_error: 9324.2432 - mean_absolute_error: 20.1959 - 5s/epoch - 20ms/step\n",
      "Epoch 113/140\n",
      "243/243 - 5s - loss: 9231.2012 - mean_squared_error: 9231.2041 - mean_absolute_error: 19.6044 - 5s/epoch - 19ms/step\n",
      "Epoch 114/140\n",
      "243/243 - 4s - loss: 9124.7627 - mean_squared_error: 9124.7637 - mean_absolute_error: 20.0570 - 4s/epoch - 19ms/step\n",
      "Epoch 115/140\n",
      "243/243 - 5s - loss: 9259.0059 - mean_squared_error: 9259.0059 - mean_absolute_error: 21.0185 - 5s/epoch - 19ms/step\n",
      "Epoch 116/140\n",
      "243/243 - 4s - loss: 9073.3799 - mean_squared_error: 9073.3789 - mean_absolute_error: 21.1208 - 4s/epoch - 18ms/step\n",
      "Epoch 117/140\n",
      "243/243 - 4s - loss: 8950.3291 - mean_squared_error: 8950.3320 - mean_absolute_error: 20.6985 - 4s/epoch - 18ms/step\n",
      "Epoch 118/140\n",
      "243/243 - 4s - loss: 9048.5547 - mean_squared_error: 9048.5596 - mean_absolute_error: 19.1245 - 4s/epoch - 18ms/step\n",
      "Epoch 119/140\n",
      "243/243 - 4s - loss: 8825.7627 - mean_squared_error: 8825.7617 - mean_absolute_error: 20.3088 - 4s/epoch - 18ms/step\n",
      "Epoch 120/140\n",
      "243/243 - 4s - loss: 9127.1816 - mean_squared_error: 9127.1816 - mean_absolute_error: 18.3178 - 4s/epoch - 18ms/step\n",
      "Epoch 121/140\n",
      "243/243 - 5s - loss: 9202.8486 - mean_squared_error: 9202.8486 - mean_absolute_error: 18.0747 - 5s/epoch - 19ms/step\n",
      "Epoch 122/140\n",
      "243/243 - 4s - loss: 9398.3545 - mean_squared_error: 9398.3574 - mean_absolute_error: 20.9304 - 4s/epoch - 18ms/step\n",
      "Epoch 123/140\n",
      "243/243 - 4s - loss: 9483.8105 - mean_squared_error: 9483.8125 - mean_absolute_error: 17.7233 - 4s/epoch - 18ms/step\n",
      "Epoch 124/140\n",
      "243/243 - 5s - loss: 9274.6377 - mean_squared_error: 9274.6377 - mean_absolute_error: 18.6746 - 5s/epoch - 19ms/step\n",
      "Epoch 125/140\n",
      "243/243 - 5s - loss: 9212.0010 - mean_squared_error: 9212.0020 - mean_absolute_error: 20.1652 - 5s/epoch - 19ms/step\n",
      "Epoch 126/140\n",
      "243/243 - 5s - loss: 9193.1006 - mean_squared_error: 9193.0996 - mean_absolute_error: 21.8097 - 5s/epoch - 19ms/step\n",
      "Epoch 127/140\n",
      "243/243 - 5s - loss: 9228.4893 - mean_squared_error: 9228.4883 - mean_absolute_error: 21.0119 - 5s/epoch - 19ms/step\n",
      "Epoch 128/140\n",
      "243/243 - 5s - loss: 9021.9111 - mean_squared_error: 9021.9043 - mean_absolute_error: 20.8434 - 5s/epoch - 19ms/step\n",
      "Epoch 129/140\n",
      "243/243 - 5s - loss: 8929.1396 - mean_squared_error: 8929.1377 - mean_absolute_error: 21.1974 - 5s/epoch - 19ms/step\n",
      "Epoch 130/140\n",
      "243/243 - 5s - loss: 9036.0625 - mean_squared_error: 9036.0625 - mean_absolute_error: 20.0389 - 5s/epoch - 20ms/step\n",
      "Epoch 131/140\n",
      "243/243 - 5s - loss: 8883.2744 - mean_squared_error: 8883.2773 - mean_absolute_error: 20.6378 - 5s/epoch - 19ms/step\n",
      "Epoch 132/140\n",
      "243/243 - 6s - loss: 8689.5459 - mean_squared_error: 8689.5479 - mean_absolute_error: 19.6323 - 6s/epoch - 24ms/step\n",
      "Epoch 133/140\n",
      "243/243 - 5s - loss: 8843.0225 - mean_squared_error: 8843.0225 - mean_absolute_error: 21.2699 - 5s/epoch - 21ms/step\n",
      "Epoch 134/140\n",
      "243/243 - 5s - loss: 8797.6309 - mean_squared_error: 8797.6230 - mean_absolute_error: 19.7034 - 5s/epoch - 20ms/step\n",
      "Epoch 135/140\n",
      "243/243 - 5s - loss: 8990.5938 - mean_squared_error: 8990.5957 - mean_absolute_error: 20.6624 - 5s/epoch - 19ms/step\n",
      "Epoch 136/140\n",
      "243/243 - 5s - loss: 9331.0908 - mean_squared_error: 9331.0938 - mean_absolute_error: 17.5596 - 5s/epoch - 19ms/step\n",
      "Epoch 137/140\n",
      "243/243 - 5s - loss: 9201.9385 - mean_squared_error: 9201.9404 - mean_absolute_error: 18.6425 - 5s/epoch - 20ms/step\n",
      "Epoch 138/140\n",
      "243/243 - 5s - loss: 9019.4658 - mean_squared_error: 9019.4668 - mean_absolute_error: 18.3369 - 5s/epoch - 20ms/step\n",
      "Epoch 139/140\n",
      "243/243 - 5s - loss: 8969.5312 - mean_squared_error: 8969.5352 - mean_absolute_error: 19.4792 - 5s/epoch - 20ms/step\n",
      "Epoch 140/140\n",
      "243/243 - 5s - loss: 9159.4062 - mean_squared_error: 9159.4102 - mean_absolute_error: 19.9862 - 5s/epoch - 20ms/step\n"
     ]
    }
   ],
   "source": [
    "\n",
    "history = model2.fit(X_train, # input data\n",
    "                    Y_train, # target data\n",
    "                    batch_size=3, # Number of samples per gradient update. If unspecified, batch_size will default to 32.\n",
    "                    epochs=140, # default=1, Number of epochs to train the model. An epoch is an iteration over the entire x and y data provided\n",
    "                    verbose=2, # default='auto', ('auto', 0, 1, or 2). Verbosity mode. 0 = silent, 1 = progress bar, 2 = one line per epoch. 'auto' defaults to 1 for most cases, but 2 when used with ParameterServerStrategy.\n",
    "                    callbacks=None, # default=None, list of callbacks to apply during training. See tf.keras.callbacks\n",
    "                    validation_split=0.2, # default=0.0, Fraction of the training data to be used as validation data. The model will set apart this fraction of the training data, will not train on it, and will evaluate the loss and any model metrics on this data at the end of each epoch. \n",
    "                    #validation_data=(X_test, y_test), # default=None, Data on which to evaluate the loss and any model metrics at the end of each epoch. \n",
    "                    shuffle=True, # default=True, Boolean (whether to shuffle the training data before each epoch) or str (for 'batch').\n",
    "                    class_weight=None, # default=None, Optional dictionary mapping class indices (integers) to a weight (float) value, used for weighting the loss function (during training only). This can be useful to tell the model to \"pay more attention\" to samples from an under-represented class.\n",
    "                    sample_weight=None, # default=None, Optional Numpy array of weights for the training samples, used for weighting the loss function (during training only).\n",
    "                    initial_epoch=0, # Integer, default=0, Epoch at which to start training (useful for resuming a previous training run).\n",
    "                    steps_per_epoch=None, # Integer or None, default=None, Total number of steps (batches of samples) before declaring one epoch finished and starting the next epoch. When training with input tensors such as TensorFlow data tensors, the default None is equal to the number of samples in your dataset divided by the batch size, or 1 if that cannot be determined. \n",
    "                    validation_steps=None, # Only relevant if validation_data is provided and is a tf.data dataset. Total number of steps (batches of samples) to draw before stopping when performing validation at the end of every epoch.\n",
    "                    validation_batch_size=None, # Integer or None, default=None, Number of samples per validation batch. If unspecified, will default to batch_size.\n",
    "                    validation_freq=100, # default=1, Only relevant if validation data is provided. If an integer, specifies how many training epochs to run before a new validation run is performed, e.g. validation_freq=2 runs validation every 2 epochs.\n",
    "                    max_queue_size=10, # default=10, Used for generator or keras.utils.Sequence input only. Maximum size for the generator queue. If unspecified, max_queue_size will default to 10.\n",
    "                    workers=1, # default=1, Used for generator or keras.utils.Sequence input only. Maximum number of processes to spin up when using process-based threading. If unspecified, workers will default to 1.\n",
    "                    use_multiprocessing=True, # default=False, Used for generator or keras.utils.Sequence input only. If True, use process-based threading. If unspecified, use_multiprocessing will default to False. \n",
    "                \n",
    "                   )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [9477.8798828125,\n",
       "  9413.4462890625,\n",
       "  9381.3857421875,\n",
       "  9310.232421875,\n",
       "  9292.673828125,\n",
       "  9316.7646484375,\n",
       "  9268.2177734375,\n",
       "  9164.908203125,\n",
       "  9165.5859375,\n",
       "  9054.0517578125,\n",
       "  15310.3388671875,\n",
       "  9485.6708984375,\n",
       "  9253.0029296875,\n",
       "  9387.8291015625,\n",
       "  9940.12109375,\n",
       "  9146.5224609375,\n",
       "  9121.7353515625,\n",
       "  9044.2158203125,\n",
       "  8959.8251953125,\n",
       "  8966.75390625,\n",
       "  8891.4921875,\n",
       "  8888.3935546875,\n",
       "  8851.2177734375,\n",
       "  8771.1171875,\n",
       "  9084.095703125,\n",
       "  8759.068359375,\n",
       "  8537.8671875,\n",
       "  9164.4921875,\n",
       "  9389.8759765625,\n",
       "  9319.822265625,\n",
       "  9230.83984375,\n",
       "  9127.2451171875,\n",
       "  9111.7607421875,\n",
       "  8796.8232421875,\n",
       "  9189.6533203125,\n",
       "  8795.193359375,\n",
       "  8953.2373046875,\n",
       "  8907.197265625,\n",
       "  8588.14453125,\n",
       "  8349.4892578125,\n",
       "  8742.06640625,\n",
       "  8558.2978515625,\n",
       "  8821.8837890625,\n",
       "  8660.5361328125,\n",
       "  8831.8876953125,\n",
       "  9361.9189453125,\n",
       "  9268.720703125,\n",
       "  9148.123046875,\n",
       "  8923.75390625,\n",
       "  9224.2138671875,\n",
       "  8841.44140625,\n",
       "  8621.08984375,\n",
       "  8445.2373046875,\n",
       "  8338.6513671875,\n",
       "  8083.32421875,\n",
       "  8881.791015625,\n",
       "  9430.529296875,\n",
       "  9255.7646484375,\n",
       "  9178.330078125,\n",
       "  9140.9638671875,\n",
       "  8928.2490234375,\n",
       "  8458.6640625,\n",
       "  8699.9990234375,\n",
       "  9418.296875,\n",
       "  9296.3876953125,\n",
       "  9194.5546875,\n",
       "  9421.66796875,\n",
       "  9325.78125,\n",
       "  9141.0439453125,\n",
       "  9440.7421875,\n",
       "  9259.423828125,\n",
       "  9163.2353515625,\n",
       "  9158.556640625,\n",
       "  9136.658203125,\n",
       "  8960.5986328125,\n",
       "  9100.0947265625,\n",
       "  8916.4140625,\n",
       "  9109.1494140625,\n",
       "  9117.96875,\n",
       "  9278.6259765625,\n",
       "  9205.13671875,\n",
       "  9057.3447265625,\n",
       "  8953.966796875,\n",
       "  8937.056640625,\n",
       "  8537.8857421875,\n",
       "  8281.51171875,\n",
       "  8239.6748046875,\n",
       "  8351.34765625,\n",
       "  8491.490234375,\n",
       "  7928.0234375,\n",
       "  8017.92041015625,\n",
       "  8573.6767578125,\n",
       "  9634.1884765625,\n",
       "  9356.5947265625,\n",
       "  9374.869140625,\n",
       "  9300.8046875,\n",
       "  9204.0634765625,\n",
       "  9855.86328125,\n",
       "  9390.345703125,\n",
       "  9853.05859375,\n",
       "  9436.8232421875,\n",
       "  9377.3369140625,\n",
       "  13855.6044921875,\n",
       "  9342.8232421875,\n",
       "  9280.7958984375,\n",
       "  9271.353515625,\n",
       "  9303.75390625,\n",
       "  9269.2978515625,\n",
       "  9242.99609375,\n",
       "  9233.8203125,\n",
       "  9248.21875,\n",
       "  9324.2412109375,\n",
       "  9231.201171875,\n",
       "  9124.7626953125,\n",
       "  9259.005859375,\n",
       "  9073.3798828125,\n",
       "  8950.3291015625,\n",
       "  9048.5546875,\n",
       "  8825.7626953125,\n",
       "  9127.181640625,\n",
       "  9202.8486328125,\n",
       "  9398.3544921875,\n",
       "  9483.810546875,\n",
       "  9274.6376953125,\n",
       "  9212.0009765625,\n",
       "  9193.1005859375,\n",
       "  9228.4892578125,\n",
       "  9021.9111328125,\n",
       "  8929.1396484375,\n",
       "  9036.0625,\n",
       "  8883.2744140625,\n",
       "  8689.5458984375,\n",
       "  8843.0224609375,\n",
       "  8797.630859375,\n",
       "  8990.59375,\n",
       "  9331.0908203125,\n",
       "  9201.9384765625,\n",
       "  9019.4658203125,\n",
       "  8969.53125,\n",
       "  9159.40625],\n",
       " 'mean_squared_error': [9477.8857421875,\n",
       "  9413.447265625,\n",
       "  9381.3837890625,\n",
       "  9310.2353515625,\n",
       "  9292.6767578125,\n",
       "  9316.7626953125,\n",
       "  9268.220703125,\n",
       "  9164.9091796875,\n",
       "  9165.583984375,\n",
       "  9054.0537109375,\n",
       "  15310.3427734375,\n",
       "  9485.6767578125,\n",
       "  9253.0048828125,\n",
       "  9387.828125,\n",
       "  9940.119140625,\n",
       "  9146.51953125,\n",
       "  9121.734375,\n",
       "  9044.2158203125,\n",
       "  8959.826171875,\n",
       "  8966.751953125,\n",
       "  8891.4921875,\n",
       "  8888.388671875,\n",
       "  8851.2177734375,\n",
       "  8771.1240234375,\n",
       "  9084.09765625,\n",
       "  8759.0673828125,\n",
       "  8537.86328125,\n",
       "  9164.4921875,\n",
       "  9389.8720703125,\n",
       "  9319.8251953125,\n",
       "  9230.8427734375,\n",
       "  9127.2431640625,\n",
       "  9111.7568359375,\n",
       "  8796.82421875,\n",
       "  9189.654296875,\n",
       "  8795.197265625,\n",
       "  8953.2392578125,\n",
       "  8907.1982421875,\n",
       "  8588.1435546875,\n",
       "  8349.48828125,\n",
       "  8742.0673828125,\n",
       "  8558.298828125,\n",
       "  8821.884765625,\n",
       "  8660.5341796875,\n",
       "  8831.8837890625,\n",
       "  9361.921875,\n",
       "  9268.71875,\n",
       "  9148.125,\n",
       "  8923.7529296875,\n",
       "  9224.2119140625,\n",
       "  8841.4384765625,\n",
       "  8621.0947265625,\n",
       "  8445.2353515625,\n",
       "  8338.6513671875,\n",
       "  8083.318359375,\n",
       "  8881.7919921875,\n",
       "  9430.53125,\n",
       "  9255.7607421875,\n",
       "  9178.33203125,\n",
       "  9140.962890625,\n",
       "  8928.25390625,\n",
       "  8458.6640625,\n",
       "  8700.001953125,\n",
       "  9418.30078125,\n",
       "  9296.3935546875,\n",
       "  9194.5556640625,\n",
       "  9421.6669921875,\n",
       "  9325.7802734375,\n",
       "  9141.0478515625,\n",
       "  9440.748046875,\n",
       "  9259.4228515625,\n",
       "  9163.23828125,\n",
       "  9158.5625,\n",
       "  9136.658203125,\n",
       "  8960.595703125,\n",
       "  9100.099609375,\n",
       "  8916.4140625,\n",
       "  9109.150390625,\n",
       "  9117.9736328125,\n",
       "  9278.6318359375,\n",
       "  9205.1376953125,\n",
       "  9057.345703125,\n",
       "  8953.9677734375,\n",
       "  8937.0615234375,\n",
       "  8537.8828125,\n",
       "  8281.513671875,\n",
       "  8239.6728515625,\n",
       "  8351.3486328125,\n",
       "  8491.49609375,\n",
       "  7928.02685546875,\n",
       "  8017.92236328125,\n",
       "  8573.6767578125,\n",
       "  9634.1943359375,\n",
       "  9356.5947265625,\n",
       "  9374.8671875,\n",
       "  9300.8046875,\n",
       "  9204.0673828125,\n",
       "  9855.8671875,\n",
       "  9390.34375,\n",
       "  9853.0556640625,\n",
       "  9436.8251953125,\n",
       "  9377.3369140625,\n",
       "  13855.6044921875,\n",
       "  9342.814453125,\n",
       "  9280.7958984375,\n",
       "  9271.3564453125,\n",
       "  9303.7568359375,\n",
       "  9269.296875,\n",
       "  9242.9921875,\n",
       "  9233.818359375,\n",
       "  9248.2177734375,\n",
       "  9324.2431640625,\n",
       "  9231.2041015625,\n",
       "  9124.763671875,\n",
       "  9259.005859375,\n",
       "  9073.37890625,\n",
       "  8950.33203125,\n",
       "  9048.5595703125,\n",
       "  8825.76171875,\n",
       "  9127.181640625,\n",
       "  9202.8486328125,\n",
       "  9398.357421875,\n",
       "  9483.8125,\n",
       "  9274.6376953125,\n",
       "  9212.001953125,\n",
       "  9193.099609375,\n",
       "  9228.48828125,\n",
       "  9021.904296875,\n",
       "  8929.1376953125,\n",
       "  9036.0625,\n",
       "  8883.27734375,\n",
       "  8689.5478515625,\n",
       "  8843.0224609375,\n",
       "  8797.623046875,\n",
       "  8990.595703125,\n",
       "  9331.09375,\n",
       "  9201.9404296875,\n",
       "  9019.466796875,\n",
       "  8969.53515625,\n",
       "  9159.41015625],\n",
       " 'mean_absolute_error': [15.501599311828613,\n",
       "  17.533212661743164,\n",
       "  18.298267364501953,\n",
       "  18.2609920501709,\n",
       "  18.527170181274414,\n",
       "  18.720138549804688,\n",
       "  17.948013305664062,\n",
       "  18.24146270751953,\n",
       "  18.985980987548828,\n",
       "  18.687864303588867,\n",
       "  18.998821258544922,\n",
       "  16.7475528717041,\n",
       "  17.834806442260742,\n",
       "  18.587739944458008,\n",
       "  17.300731658935547,\n",
       "  18.32595443725586,\n",
       "  18.943777084350586,\n",
       "  18.152206420898438,\n",
       "  18.64898109436035,\n",
       "  18.885814666748047,\n",
       "  18.588836669921875,\n",
       "  18.07500648498535,\n",
       "  18.634767532348633,\n",
       "  18.258636474609375,\n",
       "  16.922882080078125,\n",
       "  19.19345474243164,\n",
       "  18.912490844726562,\n",
       "  17.799442291259766,\n",
       "  16.7819766998291,\n",
       "  18.005146026611328,\n",
       "  18.608224868774414,\n",
       "  19.028186798095703,\n",
       "  20.527828216552734,\n",
       "  20.50957489013672,\n",
       "  19.059120178222656,\n",
       "  19.826515197753906,\n",
       "  19.298843383789062,\n",
       "  21.121490478515625,\n",
       "  19.178922653198242,\n",
       "  20.182804107666016,\n",
       "  20.75006103515625,\n",
       "  19.937774658203125,\n",
       "  18.702348709106445,\n",
       "  18.656341552734375,\n",
       "  20.07889175415039,\n",
       "  19.707191467285156,\n",
       "  19.999534606933594,\n",
       "  18.90043067932129,\n",
       "  18.497608184814453,\n",
       "  19.950267791748047,\n",
       "  19.277620315551758,\n",
       "  19.533239364624023,\n",
       "  19.681196212768555,\n",
       "  18.635406494140625,\n",
       "  19.401771545410156,\n",
       "  19.152313232421875,\n",
       "  19.239791870117188,\n",
       "  19.623376846313477,\n",
       "  20.22659683227539,\n",
       "  21.66875457763672,\n",
       "  20.464799880981445,\n",
       "  19.44061279296875,\n",
       "  18.756179809570312,\n",
       "  18.263227462768555,\n",
       "  19.428354263305664,\n",
       "  19.482580184936523,\n",
       "  19.848129272460938,\n",
       "  19.104707717895508,\n",
       "  20.53864860534668,\n",
       "  20.55012321472168,\n",
       "  20.070865631103516,\n",
       "  19.68326759338379,\n",
       "  21.257747650146484,\n",
       "  19.42304229736328,\n",
       "  19.438396453857422,\n",
       "  19.54501724243164,\n",
       "  19.981496810913086,\n",
       "  19.520484924316406,\n",
       "  21.846155166625977,\n",
       "  21.635486602783203,\n",
       "  19.487464904785156,\n",
       "  19.9197998046875,\n",
       "  20.60526466369629,\n",
       "  20.84689712524414,\n",
       "  19.735843658447266,\n",
       "  18.725378036499023,\n",
       "  19.07965087890625,\n",
       "  19.49390411376953,\n",
       "  18.63816261291504,\n",
       "  17.70553207397461,\n",
       "  18.215219497680664,\n",
       "  19.35507583618164,\n",
       "  19.959123611450195,\n",
       "  17.17624282836914,\n",
       "  16.99431610107422,\n",
       "  17.802780151367188,\n",
       "  19.69841957092285,\n",
       "  17.943342208862305,\n",
       "  16.53968048095703,\n",
       "  21.192089080810547,\n",
       "  20.843751907348633,\n",
       "  20.128124237060547,\n",
       "  26.726032257080078,\n",
       "  19.91092872619629,\n",
       "  18.83502197265625,\n",
       "  18.058879852294922,\n",
       "  18.15180015563965,\n",
       "  18.59165382385254,\n",
       "  19.98820686340332,\n",
       "  19.735658645629883,\n",
       "  21.991289138793945,\n",
       "  20.195907592773438,\n",
       "  19.6043701171875,\n",
       "  20.057039260864258,\n",
       "  21.01848793029785,\n",
       "  21.120807647705078,\n",
       "  20.698461532592773,\n",
       "  19.124500274658203,\n",
       "  20.30879783630371,\n",
       "  18.317846298217773,\n",
       "  18.074668884277344,\n",
       "  20.930395126342773,\n",
       "  17.72328758239746,\n",
       "  18.674560546875,\n",
       "  20.165203094482422,\n",
       "  21.80971908569336,\n",
       "  21.011873245239258,\n",
       "  20.843358993530273,\n",
       "  21.197399139404297,\n",
       "  20.03889274597168,\n",
       "  20.63780975341797,\n",
       "  19.63226890563965,\n",
       "  21.26985740661621,\n",
       "  19.703397750854492,\n",
       "  20.66236114501953,\n",
       "  17.559555053710938,\n",
       "  18.6424617767334,\n",
       "  18.33685874938965,\n",
       "  19.479188919067383,\n",
       "  19.986225128173828],\n",
       " 'val_loss': [10865.3779296875],\n",
       " 'val_mean_squared_error': [10865.3779296875],\n",
       " 'val_mean_absolute_error': [18.699024200439453]}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 4 - Plot loss over epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Gather the trained model's weight and bias.\n",
    "trained_weight = model2.get_weights()[0]\n",
    "trained_bias = model2.get_weights()[1]\n",
    "\n",
    "  # The list of epochs is stored separately from the \n",
    "  # rest of history.\n",
    "epochs = history.epoch\n",
    "  \n",
    "  # Gather the history (a snapshot) of each epoch.\n",
    "#hist = pd.DataFrame(history.history)\n",
    "\n",
    "  # Specifically gather the model's root mean \n",
    "  # squared error at each epoch. \n",
    "rmse = history.history[\"mean_squared_error\"]\n",
    "\n",
    "#plot_the_model(trained_weight, trained_bias, X_test, Y_train)\n",
    "plot_the_loss_curve(epochs, rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 5 - Print Performance Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------------------- Model Summary --------------------\n",
      "Model: \"LSTM-Model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " masking_2 (Masking)         (None, 10, 1)             0         \n",
      "                                                                 \n",
      " Input-Layer (InputLayer)    multiple                  0         \n",
      "                                                                 \n",
      " Hidden-LSTM-Encoder-Layer (  (None, 128)              33792     \n",
      " Bidirectional)                                                  \n",
      "                                                                 \n",
      " Repeat-Vector-Layer (Repeat  (None, 10, 128)          0         \n",
      " Vector)                                                         \n",
      "                                                                 \n",
      " Hidden-LSTM-Decoder-Layer (  (None, 10, 128)          98816     \n",
      " Bidirectional)                                                  \n",
      "                                                                 \n",
      " Output-Layer (TimeDistribut  (None, 10, 1)            129       \n",
      " ed)                                                             \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 132,737\n",
      "Trainable params: 132,737\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "-------------------- Weights and Biases --------------------\n",
      "Too many parameters to print but you can use the code provided if needed\n",
      "\n",
      "-------------------- Evaluation on Training Data --------------------\n",
      "Final loss : 9159.40625\n",
      "Final mean_squared_error : 9159.41015625\n",
      "Final mean_absolute_error : 19.986225128173828\n",
      "Final val_loss : 10865.3779296875\n",
      "Final val_mean_squared_error : 10865.3779296875\n",
      "Final val_mean_absolute_error : 18.699024200439453\n",
      "\n",
      "-------------------- Evaluation on Test Data --------------------\n",
      "29/29 [==============================] - 0s 14ms/step - loss: 9426.6699 - mean_squared_error: 9426.6719 - mean_absolute_error: 22.6781\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\")\n",
    "print('-------------------- Model Summary --------------------')\n",
    "model2.summary() # print model summary\n",
    "print(\"\")\n",
    "print('-------------------- Weights and Biases --------------------')\n",
    "print(\"Too many parameters to print but you can use the code provided if needed\")\n",
    "print(\"\")\n",
    "#for layer in model.layers:\n",
    "#    print(layer.name)\n",
    "#    for item in layer.get_weights():\n",
    "#        print(\"  \", item)\n",
    "#print(\"\")\n",
    "\n",
    "# Print the last value in the evaluation metrics contained within history file\n",
    "print('-------------------- Evaluation on Training Data --------------------')\n",
    "for item in history.history:\n",
    "    print(\"Final\", item, \":\", history.history[item][-1])\n",
    "print(\"\")\n",
    "\n",
    "# Evaluate the model on the test data using \"evaluate\"\n",
    "print('-------------------- Evaluation on Test Data --------------------')\n",
    "results = model2.evaluate(X_train, Y_train)\n",
    "print(\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "d3e10ef16274dd72e574b8fa73b58450b957d8421a2901baded3cca26fcf5dda"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
